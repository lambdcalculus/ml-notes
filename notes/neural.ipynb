{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4e1de2f-04e8-41bf-8130-c93d237d1af1",
   "metadata": {},
   "source": [
    "# Neural Networks: The Big Guns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c79b0-4d74-41b5-a05d-5381c48f79e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "config = {\n",
    "'axes.spines.right': False,\n",
    "'axes.spines.top': False,\n",
    "'axes.edgecolor': '.4',\n",
    "'axes.labelcolor': '.0',\n",
    "'axes.titlesize': 'large',\n",
    "'axes.labelsize': 'large',\n",
    "'figure.autolayout': True,\n",
    "'figure.figsize': (8, 5),\n",
    "'font.family': ['serif'],\n",
    "'font.size': 11.0,\n",
    "'grid.linestyle': '--',\n",
    "'legend.facecolor': '.9',\n",
    "'legend.frameon': True,\n",
    "'legend.fontsize': 'medium',\n",
    "'savefig.transparent': True,\n",
    "'text.color': '.0',\n",
    "'xtick.labelsize': 'medium',\n",
    "'ytick.labelsize': 'medium',\n",
    "}\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-whitegrid', 'seaborn-v0_8-paper', 'seaborn-v0_8-muted', config])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e95c23-514c-47fb-9b8f-b52976f4b82d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "Linear and logistic regression might work well on problems where there aren't an excessive amount of variables and there is a somewhat clean way to relate them to the data. If we'd used as inputs not *data* about the flowers but actual *pictures* of the flowers, our softmax algorithm back there would have failed pretty hard. Even if we somehow managed to get it to fit the training data, there'd be a yet tinier chance that it would generalize to the testing data.\n",
    "\n",
    "But it would undeniably be nice to have something that could do that, but... how would we even go about making it? If we could get something that could get bigger patterns in the picture and turn them into something more edible for softmax, that would be a nice start. However, let's hold that thought for a while, and talk about something else.\n",
    "\n",
    "### The Perceptron\n",
    "\n",
    "The Perceptron is a small generalization of the algorithms we've been doing so far. It is a single unit that contains many weights, and when given an input, outputs a linear combination of the weights and the inputs. However, it doesn't do just that: after the summation, it will apply an *activation function*, which is supposed to indicate how 'activated' the Perceptron is with that input. In the binary classification case, we used the sigmoid as our activation function. In linear regression, we don't really apply any function after the summation, so we can say the activation function there is simply the identity function.\n",
    "\n",
    "In the case of softmax, our output wasn't a single real number, but the same principle applied, we made the summation of inputs and weights, and then applied the softmax over that.\n",
    "\n",
    "**note:** the weight that gets multiplied with 1 has a special name. It is called the *bias*.\n",
    "\n",
    "![perceptron](img/perceptron.avif)\n",
    "\n",
    "Now why is this relevant? While a single perceptron can be decently powerful (as we've seen with linear, logistic, and softmax), it can't really do the image classification task we thought about earlier. Well, it can't do that *alone*, that is. But what if we chain multiple perceptrons together?\n",
    "\n",
    "![multi-layer perceptron with one hidden layer](img/mlp.svg)\n",
    "\n",
    "It might be surprising, but this can actually be very, *very* powerful. You might say \"duh, you add a lot of a thing, you make it more powerful\" - but that wouldn't be quite right. These *networks* of perceptrons are actually obscenely good; better than you'd think by merely looking at the sum of their parts. Just so you have an idea, adding a *single* intermediate layer (which we call *hidden layer* of perceptrons) between our inputs and outputs is [mathematically proven](https://en.wikipedia.org/wiki/Universal_approximation_theorem) to be enough to locally approximate any continuous function, with a condition, that I'll specify in a just a second. Before that, let's go on briefly about *how* might that even be possible.\n",
    "\n",
    "Well, the first thing we should notice is that it isn't always possible. If your activation functions are all linear, then the entire network could be compressed into a single linear function, undoing the whole point of having many layers. That would be bad, since now we can't replicate non-linear behavior. So you'll generally see people saying that your activation function needs to be \"non-linear.\" And that kind of makes sense - if your function is crazy enough, you can probably put together piles of it to approximate anything you want, even if the way you combine them is just via scaling, shifting, and summing (notice that with linear functions this would just yield more linear functions).\n",
    "\n",
    "**note:** by linear here we really mean *affine*. So a linear combination of the inputs + a constant.\n",
    "\n",
    "Though, \"non-linear\" is a bit innacurate, as the theorem's requirements are that the activation function is continuous, but *not* a polynomial (after all, composing, scaling, and shifting polynomials gets you more polynomials). But, that's not really important. Still, all we need is for our function to be crazy \"enough\" that it can model whatever non-linear (but continuous) phenomenon.\n",
    "\n",
    "**TO-DO:** add more examples of activation functions. tanh, ReLU, etc.\n",
    "\n",
    "### Multi-Layer Perceptron\n",
    "\n",
    "![multi-layer perceptron with many hidden layers](img/mlp.png)\n",
    "\n",
    "The theorem, however, doesn't guarantee anything about how many perceptrons we'll need in that hidden layer. That number could be *huge*. So often times we'll use *many* hidden layers with not too many neurons, which save lots of computation. This means we generally prefer to increase the *depth* of the network, over its *width*. That's where the whole *deep learning* thing comes from! Adding depth to these networks can make them *incredibly* good. Another, cooler (at least in my opinion) name for these neural networks made up of perceptrons is *Multi-Layer Perceptrons*. But there is yet an issue to resolve: how do we even get a decent model like we were doing before?\n",
    "\n",
    "### Optimization\n",
    "\n",
    "The idea is essentially the same. We are still going to use gradient descent, but we have to be a bit smart about it. We'll still have some cost function $\\mathcal J$ (which will depend on the purpose of the model), and we'll try to minimize it.\n",
    "\n",
    "We'll do this with a 3-layer neural network first (one input, one hidden, and one output) and it'll generalize in a fairly straightforward way. \n",
    "\n",
    "I didn't show the proofs before, but generally, what was done to find the formulas I presented earlier was simply using the chain rule, and we'll also do the same here. Say we have an input vector $\\boldsymbol n = [1\\; n_1\\; n_2\\cdots n_H]^\\top$ from the last hidden layer (and that this layer has $H$ neurons), and we expect an output vector $\\boldsymbol y = [y_1\\; y_2\\; \\cdots y_K]^\\top$ at the output layer. If we put our weights on the $K\\times (D+1)$ matrix (where $\\boldsymbol w_i$ is the vector of weights that lead into the $i$-th output)\n",
    "$$\n",
    "\\boldsymbol W =\n",
    "\\begin{pmatrix}\n",
    "w_{10} & w_{11} & \\cdots & w_{1D} \\\\\n",
    "w_{20} & w_{21} & \\cdots & w_{2D} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{K0} & w_{K1} & \\cdots & w_{KD}\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol w_1^\\top \\\\\n",
    "\\boldsymbol w_2^\\top \\\\\n",
    "\\vdots \\\\\n",
    "\\boldsymbol w_K^\\top\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and our activation function is $\\phi$.\n",
    "\n",
    "We'll define $\\boldsymbol s = \\boldsymbol W \\boldsymbol n$, which is the vector that has all the weighted sums. Then our output will be $\\hat{\\boldsymbol y} = \\phi(\\boldsymbol s) = \\phi(\\boldsymbol W \\boldsymbol n)$ (where $\\phi$ is applied component-wise).\n",
    "\n",
    "We'll look at minimizing the loss function $\\mathcal L$. Minimizing the cost function we'll be much the same - this is just a bit easier to look at. Recall that our loss function effectively compares how bad $\\hat{\\boldsymbol y}$ is as an approximation of $\\boldsymbol y$. In that sense, if we see $\\boldsymbol y$ as a constant, $\\mathcal L$ can be seen as a function of $\\hat{\\boldsymbol y}$. But $\\hat{\\boldsymbol y}$ is itself a function of $\\boldsymbol s$, which is a function of $\\boldsymbol W$ (assuming the $\\boldsymbol n$ as constants).\n",
    "\n",
    "This means that, if we want to find out $\\partial\\mathcal L/\\partial\\boldsymbol W$, we can use the chain rule:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol W} =\n",
    "\\frac{\\partial \\boldsymbol s}{\\partial\\boldsymbol W}\n",
    "\\frac{\\partial\\hat{\\boldsymbol y}}{\\partial\\boldsymbol s}\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\hat{\\boldsymbol y}}\n",
    "$$\n",
    "This is a bit much, though, so let's try to look at how each $\\boldsymbol w_i$ should change first. $\\boldsymbol w_i$ only affect $s_i$, the $i$-th component of $\\boldsymbol s$, and $s_i$ will only affect $\\hat y_i$. So what we can try to find is\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol w_i} =\n",
    "\\frac{\\partial s_i}{\\partial\\boldsymbol w_i}\n",
    "\\frac{\\partial\\hat{y}_i}{\\partial s_i}\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\hat{y}_i}.\n",
    "$$\n",
    "The last part will depend on the loss function. So we should assume we should be able to compute that, as loss functions tend not to be very complicated. For example, for $\\mathcal L(\\hat{\\boldsymbol y}) = \\lVert \\boldsymbol y - \\hat{\\boldsymbol y}\\rVert^2 = (y_i - \\hat y_i)$, we get\n",
    "$$\\frac{\\partial\\mathcal L}{\\partial\\hat{y}_i} = 2(y_i - \\hat y_i).$$\n",
    "$\\partial\\hat{y}_i/\\partial s_i$ will depend on the activation function. We have $s_i = \\boldsymbol w_i^\\top \\boldsymbol n$, and $\\hat y_i = \\phi(s_i)$, so we simply end up with\n",
    "$$\\frac{\\partial\\mathcal L}{\\partial\\hat{y}_i} = \\phi'(s_i) = \\phi'(\\boldsymbol w_i^\\top \\boldsymbol n)$$\n",
    "Finally, using $s_i = \\boldsymbol w_i^\\top\\boldsymbol n$, we can see that\n",
    "$$\\frac{\\partial s_i}{\\partial\\boldsymbol w_i} = \\boldsymbol n.$$\n",
    "Hence, we end up with\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol w_i} =\n",
    "\\boldsymbol n \\cdot \\phi'(\\boldsymbol w_i^\\top \\boldsymbol n)\\cdot \\frac{\\partial\\mathcal L}{\\partial\\hat{y}_i}\n",
    "$$\n",
    "It'll be easier to get $\\partial\\mathcal L/\\partial\\boldsymbol W$ this way. First, note that $\\partial\\mathcal L/\\partial(\\boldsymbol w_i^\\top) = (\\partial\\mathcal L/\\partial\\boldsymbol w_i)^\\top$. Now, we have:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol W} =\n",
    "\\begin{pmatrix}\n",
    "(\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol w_1})^\\top \\\\\n",
    "(\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol w_2})^\\top \\\\\n",
    "\\vdots \\\\\n",
    "(\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol w_K})^\\top\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol n \\cdot \\phi'(\\boldsymbol w_1^\\top \\boldsymbol n)\\cdot \\frac{\\partial\\mathcal L}{\\partial\\hat{y}_1} \\\\\n",
    "\\boldsymbol n \\cdot \\phi'(\\boldsymbol w_2^\\top \\boldsymbol n)\\cdot \\frac{\\partial\\mathcal L}{\\partial\\hat{y}_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\boldsymbol n \\cdot \\phi'(\\boldsymbol w_K^\\top \\boldsymbol n)\\cdot \\frac{\\partial\\mathcal L}{\\partial\\hat{y}_K}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "If we put those $v_i = \\phi'(\\boldsymbol w_i^\\top \\boldsymbol n)\\cdot \\frac{\\partial\\mathcal L}{\\partial\\hat{y}_i}$ on a vector $\\boldsymbol v$, the next step might not seem too unusual, as it is very tempting to try to turn that vector into a matrix multiplication involving $\\boldsymbol n$:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol W} =\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol n \\cdot \\phi'(\\boldsymbol w_1^\\top \\boldsymbol n)\\cdot \\frac{\\partial\\mathcal L}{\\partial\\hat{y}_1} \\\\\n",
    "\\boldsymbol n \\cdot \\phi'(\\boldsymbol w_2^\\top \\boldsymbol n)\\cdot \\frac{\\partial\\mathcal L}{\\partial\\hat{y}_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\boldsymbol n \\cdot \\phi'(\\boldsymbol w_K^\\top \\boldsymbol n)\\cdot \\frac{\\partial\\mathcal L}{\\partial\\hat{y}_K}\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "\\boldsymbol n\\cdot v_1 \\\\\n",
    "\\boldsymbol n\\cdot v_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\boldsymbol n\\cdot v_K\n",
    "\\end{pmatrix} =\n",
    "\\boldsymbol v\\cdot \\boldsymbol n^\\top.\n",
    "$$\n",
    "We can also eliminate $\\boldsymbol v$ and get back the variables we started with by noting that the $\\phi'(\\boldsymbol w_i^\\top\\boldsymbol n)$ are the components of the vector $\\phi'(\\boldsymbol W\\boldsymbol n)$ and the $\\partial\\mathcal L/\\partial \\hat y_i$ are the components of the vector $\\partial\\mathcal L/\\partial\\hat{\\boldsymbol y}$. So $\\boldsymbol v$ is the element-wise product of $\\phi'(\\boldsymbol W\\boldsymbol n)$ and $\\partial\\mathcal L/\\partial \\hat{\\boldsymbol y}$! There is a special notation for that: $\\odot$. Hence, we FINALLY have:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol W} =\n",
    "\\left[\\phi'(\\boldsymbol W\\boldsymbol n)\\odot \\frac{\\partial\\mathcal L}{\\partial\\hat{\\boldsymbol y}}\\right]\\cdot\n",
    "\\boldsymbol n^\\top\n",
    "$$\n",
    "Phew! Holy crap! That took a long time, and it was just for a single layer! How are we going to be able to figure out how to do this for the deeper layers?!\n",
    "\n",
    "Calm down now. It's actually very simple after this - we've already done most of the work.\n",
    "\n",
    "### Backpropagation\n",
    "\n",
    "Say we now have $\\boldsymbol x = [1\\; x_1\\; x_2\\; \\cdots\\; x_D]^\\top$ as inputs but now they are going into a hidden layer with $H$ neurons. We'll have a vector for the perceptrons on this layer, which are effectively going to be \"outputs\" for a short time $\\boldsymbol n = [n_1\\; n_2\\; \\cdots n_H]^\\top$. Say the weights are in a $H\\times (D+1)$ matrix $\\boldsymbol M$ like before. If we define $\\boldsymbol r = \\boldsymbol M \\boldsymbol x$ as the vector that has all the weighted sums, we'll have $\\boldsymbol n = \\phi'(\\boldsymbol r) = \\phi'(\\boldsymbol M\\boldsymbol x)$ We'd like to get $\\partial\\mathcal L/\\partial\\boldsymbol M$ so we can know how to adjust the weights. Like before, by the chain rule:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol M} =\n",
    "\\frac{\\partial \\boldsymbol r}{\\partial\\boldsymbol M}\n",
    "\\frac{\\partial\\boldsymbol n}{\\partial\\boldsymbol r}\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol n}\n",
    "$$\n",
    "However, we already know what this will come out to from the work we did last time. This will be:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol M} =\n",
    "\\left[\\phi'(\\boldsymbol M\\boldsymbol x)\\odot \\frac{\\partial\\mathcal L}{\\partial\\boldsymbol n}\\right]\\cdot\n",
    "\\boldsymbol x^\\top\n",
    "$$\n",
    "A bit more palatable, but... we don't have everything we need to compute this. $\\partial\\mathcal L/\\partial\\boldsymbol n$ is something we don't know. But we can use the chain rule again:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol n} =\n",
    "\\frac{\\partial\\boldsymbol r}{\\partial\\boldsymbol n}\n",
    "\\frac{\\partial\\hat{\\boldsymbol y}}{\\partial\\boldsymbol r}\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\hat{\\boldsymbol y}}\n",
    "$$\n",
    "I'll skip the computation this time, but it's very similar to what we did last time. This comes out to:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol n} =\n",
    "\\boldsymbol W^\\top\\cdot\n",
    "\\left[\\phi'(\\boldsymbol W\\boldsymbol n)\\odot \\frac{\\partial\\mathcal L}{\\partial\\hat{\\boldsymbol y}}\\right]\n",
    "$$\n",
    "Cool. We can now adjust both $\\boldsymbol W$ and $\\boldsymbol M$, but if we had more layers, things would surely get out of hand...\n",
    "\n",
    "Now hold on a moment! Notice what we needed to compute the gradient with respect to the weights each time. For the weights between the hidden layer and the output, we needed $\\boldsymbol n$ and $\\boldsymbol W$, which can be seen as components \"owned\" by the hidden layer (*its* neurons and weights)... and we needed $\\partial\\mathcal L/\\partial\\hat{\\boldsymbol y}$, the gradient with respect to the next layer.\n",
    "\n",
    "And for the weights between the input layer and the hidden layer, we needed $\\boldsymbol x$ and $\\boldsymbol M$, which can be seen as components \"owned\" by the input layer (*its* neurons and weights)... and we needed $\\partial\\mathcal L/\\partial\\boldsymbol n$, the gradient with respect to the next layer!\n",
    "\n",
    "Looks like we have a pattern forming!\n",
    "\n",
    "Remember that our hope with studying all this is eventually programming an algorithm that will deal with these things. And right in front of us we have what we need to create that algorithm. If we think of each layer as \"owning\" the variables that represent the neurons and the variables that represent weights that extend into the next layer, then the only external thing it needs to compute how much it needs to update its weights is the gradient with respect to the *next* layer. Once it does that, we can go and update the previous layer... but then it'll need the gradient with respect to the layer we were just on.\n",
    "\n",
    "Not only that, the gradient with respect to a layer can *also* be computed with its neurons and weights, with the addition of the gradient with respect to the next layer.\n",
    "\n",
    "So what seems to be a reasonable compromise so every layer can be able to compute how much it needs to update its weights?\n",
    "\n",
    "Easy! After computing how much it needs to updates its weights, the layer computes the gradient with respect to itself as well, then passes it on to the previous layer so they can use it. This closes the chain, and propagates all the required updates to all the layers. Since this propagation is done backwards, this is called *backpropagation*.\n",
    "\n",
    "In terms that are a bit more precise:\n",
    "\n",
    "Say we have a layer $\\tt N$ and its neurons are $\\boldsymbol n = [1\\; n_1\\; n_2\\; \\cdots n_H]^\\top$. This layer is responsible for the $L\\times(H+1)$ matrix of weights $\\boldsymbol W$ that goes into the next layer $\\tt M$, whose neurons are $\\boldsymbol m = [m_1\\; m_2\\; \\cdots m_L]^\\top$. Our algorithm is as follows:\n",
    "* Backpropagation has finished on layer $\\tt M$. $\\tt M$ computed $\\partial\\mathcal L/\\partial\\boldsymbol m$ and must pass it on to the previous layer, $\\tt N$.\n",
    "* Backpropagation begins on layer $\\tt N$. It will use the value $\\partial\\mathcal L/\\partial\\boldsymbol m$ it got from the $\\tt M$ to compute:\n",
    "$$\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol W} =\n",
    "\\left[\\phi'(\\boldsymbol W\\boldsymbol n)\\odot \\frac{\\partial\\mathcal L}{\\partial\\boldsymbol m}\\right]\\cdot\n",
    "\\boldsymbol n^\\top\n",
    "\\text{, and }\n",
    "\\frac{\\partial\\mathcal L}{\\partial\\boldsymbol n} =\n",
    "\\boldsymbol W^\\top\\cdot\n",
    "\\left[\\phi'(\\boldsymbol W\\boldsymbol n)\\odot \\frac{\\partial\\mathcal L}{\\partial\\boldsymbol m}\\right]\n",
    "$$\n",
    "* Use $\\partial\\mathcal L/\\partial\\boldsymbol W$ to update $\\boldsymbol W$.\n",
    "* Backpropagation has finished on layer $\\tt N$. $\\tt N$ computed $\\partial\\mathcal L/\\partial\\boldsymbol n$ and must pass it on to the previous layer, ...\n",
    "* and so on, until we get to the input layer!\n",
    "\n",
    "This is now in a simple enough format we can implement it in code, even. Let's try to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4d851-4240-4104-843d-ddb596eb6c54",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "I'll implement the very same network that is used as an example in [3blue1brown's short series about neural networks](https://www.youtube.com/watch?v=aircAruvnKk). If you didn't like/understand my explanation (but are still here for some reason), maybe give him a try. It's not as deep (haha, get it) but it's a good primer.\n",
    "\n",
    "Now, this network does have some problems. For example, the usage of the sigmoid everywhere is not optimal. At the output layer, ideally, you'd use softmax. But you can get pretty good results with the sigmoid everywhere too. Similarly, MSE is not the optimal choice for a cost function here - cross-entropy is better for classification tasks. However, I'll just go ahead and implement it like the video - later, I might add options for your own activation or cost function.\n",
    "\n",
    "**TO-DO:** improve code\n",
    "\n",
    "**TO-DO:** add mention about how to choose initial weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38a8dc96-3e80-4a82-9707-4e3b27806a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we'll be using sigmoid as our activation always\n",
    "sigmoid = lambda x : 1 / (1 + np.exp(-x))\n",
    "d_sigmoid = lambda x : sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "class InputLayer:\n",
    "    \"\"\"The input layer of the MLP.\n",
    "    It keeps tracks of the inputs along with the weights that go into\n",
    "    the next layer (including the biases).\n",
    "    It begins the process of forward propagation, and during back propagation\n",
    "    it updates its weights (and biases) accordingly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs, l_rate=1, reg=0):\n",
    "        # inputs = [1, x_1, x_2, ..., x_D]^T\n",
    "        # not to be confused with [x_1, x_2, ..., x_D]^T\n",
    "        self.num_neurons = num_inputs\n",
    "        self.inputs = np.zeros((self.num_neurons+1, 1))\n",
    "        self.inputs[0] = 1\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.l_rate = l_rate\n",
    "        self.reg = reg\n",
    "\n",
    "        # next layer and weights are initialized later\n",
    "        self.next_layer = None\n",
    "        self.weights = None\n",
    "\n",
    "    def init_weights(self, next_layer):\n",
    "        # initialize biases to 0, others to normal distribution.\n",
    "        # H x (D+1) matrix.\n",
    "        self.next_layer = next_layer\n",
    "        self.weights = np.concatenate((\n",
    "            np.zeros((self.next_layer.num_neurons, 1)),\n",
    "            np.random.randn(\n",
    "                self.next_layer.num_neurons,\n",
    "                self.num_neurons,\n",
    "                ) * np.sqrt(1 / self.num_neurons)\n",
    "            ), axis=1\n",
    "        )\n",
    "\n",
    "    def begin(self, input):\n",
    "        \"\"\"Begin computing output.\"\"\"\n",
    "        self.inputs[1:] = np.array(input)\n",
    "        self.forward_prop()\n",
    "\n",
    "    def compute(self):\n",
    "        \"\"\"Compute weights * inputs.\"\"\"\n",
    "        return np.matmul(self.weights, self.inputs)\n",
    "\n",
    "    def forward_prop(self):\n",
    "        \"\"\"Begins forward propagation.\"\"\"\n",
    "        next = sigmoid(self.compute())\n",
    "        self.next_layer.forward_prop(next)\n",
    "\n",
    "    def back_prop(self, dJ_dy):\n",
    "        \"\"\"Updates weights, finishing back propagation.\"\"\"\n",
    "        prod = d_sigmoid(self.compute()) * dJ_dy\n",
    "        dJ_dW = np.matmul(prod, self.inputs.T)\n",
    "        self.weights = self.weights - self.l_rate * (dJ_dW + self.reg * self.weights)\n",
    "\n",
    "class HiddenLayer:\n",
    "    \"\"\"A hidden layer of the MLP.\n",
    "    It keeps track of its own neurons and the weights that go into\n",
    "    the next layer (including the biases).\n",
    "    During forward propagation it updates its neurons and carries forward\n",
    "    the signal.\n",
    "    During back propagation it updates its weights (and biases) and back\n",
    "    propagates the changes.\"\"\"\n",
    "\n",
    "    def __init__(self, num_neurons, prev_layer, l_rate=1, reg=0):\n",
    "        # neurons = [1, n_1, n_2, ..., n_H]^T\n",
    "        # not to be confused with [n_1, n_2, ..., n_H]\n",
    "        self.num_neurons = num_neurons\n",
    "        self.neurons = np.zeros((self.num_neurons+1, 1))\n",
    "        self.neurons[0] = 1\n",
    "        \n",
    "        self.prev_layer = prev_layer\n",
    "\n",
    "        # hyperparameters\n",
    "        self.l_rate = l_rate\n",
    "        self.reg = reg\n",
    "\n",
    "        # next layer and weights are initialized later\n",
    "        self.next_layer = None\n",
    "        self.weights = None\n",
    "\n",
    "    def init_weights(self, next_layer):\n",
    "        # initialize biases to 0, others to normal distribution.\n",
    "        # K x (H+1) matrix.\n",
    "        self.next_layer = next_layer\n",
    "        self.weights = np.concatenate((\n",
    "            np.zeros((self.next_layer.num_neurons, 1)),\n",
    "            np.random.randn(\n",
    "                self.next_layer.num_neurons,\n",
    "                self.num_neurons,\n",
    "                ) * np.sqrt(1 / self.num_neurons)\n",
    "            ), axis=1\n",
    "        )\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"Computes weighted summation of inputs.\"\"\"\n",
    "        return np.matmul(self.weights, self.neurons)\n",
    "\n",
    "    def forward_prop(self, prev):\n",
    "        \"\"\"Updates neurons and forward propagates.\"\"\"\n",
    "        self.neurons[1:] = prev\n",
    "        next = sigmoid(self.compute())\n",
    "        \n",
    "        self.next_layer.forward_prop(next)\n",
    "\n",
    "    def back_prop(self, dJ_dy):\n",
    "        \"\"\"Updates weights and passes along gradient with respect\n",
    "        to neurons so the previous layer can use it in its own\n",
    "        computation.\"\"\"\n",
    "        prod = d_sigmoid(self.compute()) * dJ_dy\n",
    "        dJ_dn = np.matmul(self.weights.T, prod)\n",
    "        dJ_dW = np.matmul(prod, self.neurons.T)\n",
    "        self.weights = self.weights - self.l_rate * (dJ_dW + self.reg * self.weights)\n",
    "        \n",
    "        # we don't need the 1 at the start, so we throw it out\n",
    "        # before passing dJ_dn backwards\n",
    "        self.prev_layer.back_prop(dJ_dn[1:])\n",
    "        \n",
    "class OutputLayer:\n",
    "    \"\"\"The output layer of the MLP.\n",
    "    Its only task is storing the outputs and beginning back propagation.\"\"\"\n",
    "\n",
    "    def __init__(self, num_outputs, prev_layer, l_rate=1, reg=0):\n",
    "        # outputs = [y_1, y_2, ..., y_K]^T\n",
    "        # not to be confused with [1, y_1, y_2, ..., y_K]^T\n",
    "        self.num_neurons = num_outputs\n",
    "        self.outputs = np.zeros((self.num_neurons, 1))\n",
    "\n",
    "        self.prev_layer = prev_layer\n",
    "\n",
    "        # hyperparameters\n",
    "        self.l_rate = l_rate\n",
    "        self.reg = reg\n",
    "\n",
    "    def forward_prop(self, prev):\n",
    "        \"\"\"Finishes forward propagation.\"\"\"\n",
    "        self.outputs = prev\n",
    "\n",
    "    def back_prop(self, dJ_dy):\n",
    "        \"\"\"Update weights and continue back propagation.\n",
    "        First gradient must be computed outside, by the MLP.\"\"\"\n",
    "        # we have no knowledge of the cost function,\n",
    "        # we simply pass along the value we get for the gradient\n",
    "        self.prev_layer.back_prop(dJ_dy)\n",
    "\n",
    "class Network:\n",
    "    \"\"\"A multi-layer perceptron.\n",
    "    As it is, it has two hidden layers along with its input and output layer.\n",
    "    Number of inputs, outputs, and neurons on the hidden layers can be customized,\n",
    "    along with hyperparameters.\n",
    "    It can begin feedforward to get an output, or train on given data.\"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, num_hidden=16, l_rate=1, reg=0):\n",
    "        # hyperparemeters\n",
    "        self.l_rate = l_rate\n",
    "        self.reg = reg\n",
    "\n",
    "        # create layers\n",
    "        self.input_layer = InputLayer(num_inputs, l_rate=l_rate, reg=reg)\n",
    "        self.hidden_layer1 = HiddenLayer(num_hidden, self.input_layer, l_rate=l_rate, reg=reg)\n",
    "        self.hidden_layer2 = HiddenLayer(num_hidden, self.hidden_layer1, l_rate=l_rate, reg=reg)\n",
    "        self.output_layer = OutputLayer(num_outputs, self.hidden_layer2, l_rate=l_rate, reg=reg)\n",
    "\n",
    "        # initialize weights\n",
    "        self.input_layer.init_weights(self.hidden_layer1)\n",
    "        self.hidden_layer1.init_weights(self.hidden_layer2)\n",
    "        self.hidden_layer2.init_weights(self.output_layer)\n",
    "\n",
    "    def run_single(self, input):\n",
    "        \"\"\"Computes output with current weights, on a single input.\"\"\"\n",
    "        self.input_layer.begin(input)\n",
    "        return self.output_layer.outputs\n",
    "\n",
    "    def run_batch(self, input_batch):\n",
    "        \"\"\"Computes output on a batch of inputs.\"\"\"\n",
    "        return np.array([self.run_single(input) for input in input_batch])\n",
    "\n",
    "    def back_prop(self, dJ_dy):\n",
    "        \"\"\"Begins back-propagation.\n",
    "        First gradient must come from one of the train methods,\n",
    "        as we might train on single data or on batches.\"\"\"\n",
    "        self.output_layer.back_prop(dJ_dy)\n",
    "\n",
    "    # the training functions returning the cost is a bit of an idiosyncracy,\n",
    "    # but it helps with efficiency so that we don't end up making too many\n",
    "    # computations to get the cost (e.g. a get_cost function would\n",
    "    # need to run the model on the outputs as well)\n",
    "    def train_single(self, x_train, y_train):\n",
    "        \"\"\"Applies gradient descent on a single input/output pair.\n",
    "        Also returns the cost as it was before training.\"\"\"\n",
    "        y_hat = self.run_single(x_train)\n",
    "        cost = np.sum((y_train - y_hat)**2)\n",
    "        dJ_dy = 2 * (y_hat - y_train)\n",
    "        self.back_prop(dJ_dy)\n",
    "        return cost\n",
    "\n",
    "    def train_batch(self, x_batch, y_batch):\n",
    "        \"\"\"Applies gradient descent to a batch of input/output pairs.\n",
    "        Also returns the cost as it was before training.\"\"\"\n",
    "        y_expected = np.array(y_batch)\n",
    "        y_hat_batch = self.run_batch(x_batch)\n",
    "        N = len(y_hat_batch)\n",
    "        cost = 1/N * np.sum((y_hat_batch - y_expected)**2)\n",
    "        dJ_dy = 2/N * np.sum(y_hat_batch - y_expected, axis = 0)\n",
    "        self.back_prop(dJ_dy)\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d98c6-cae5-4c71-9320-73fb0125cc3a",
   "metadata": {},
   "source": [
    "### The task\n",
    "\n",
    "Our task will be classifying $28\\times 28$ grayscale images of numbers from 0 to 9 into... numbers from 0 to 9. This means our input layer will have $28^2=784$ inputs! Even so, $16$ neurons on each hidden layer will suffice for very good results, as we'll see. Our output will be 10 floats from 0 to 1 indicating how confident the model is that the picture is a given number. When checking for accuracy, we can say the output we get is whichever number has the highest confidence.\n",
    "\n",
    "The dataset will be the famous MNIST data set, also used in the 3b1b video.\n",
    "\n",
    "I'll train it with mini-batches, since the dataset is pretty huge (60k images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e67c3536-1bf7-4ade-ba1a-e0eab3531400",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "# getting the testing and training data\n",
    "(x_train_data, y_train_data), (x_test_data, y_test_data) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e38abcb8-beed-4b34-aac2-65518ccf4b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training...\n",
      "EPOCH 200 out of 200 (600/600)  \n",
      "Training Done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "# hyperparameters we'll be using\n",
    "l_rate = 1.0\n",
    "reg = 0.000\n",
    "\n",
    "# normalizing and formatting data\n",
    "x_train = (np.array(x_train_data) / 255).reshape(len(x_train_data), 28*28, 1)\n",
    "format = lambda x : np.array([1 if i == x else 0 for i in range(10)]).reshape(10, 1)\n",
    "y_train = np.array([format(y) for y in y_train_data])\n",
    "\n",
    "# initializing the network\n",
    "net = Network(28*28, 10, l_rate=l_rate, reg=reg)\n",
    "\n",
    "# training parameters\n",
    "batch_size = 100\n",
    "epochs = 200\n",
    "\n",
    "# we'll collect the costs for a graph later\n",
    "costs = []\n",
    "\n",
    "# put the data in a shape we can shuffle\n",
    "data = [(x_train[i], y_train[i]) for i in range(len(x_train))]\n",
    "\n",
    "print(\"Beginning training...\")\n",
    "for epoch in range(1, epochs+1):\n",
    "    # shuffle data\n",
    "    shuffle(data)\n",
    "    x_train = np.array([data[i][0] for i in range(len(x_train))])\n",
    "    y_train = np.array([data[i][1] for i in range(len(y_train))])\n",
    "\n",
    "    # initialize batch vars\n",
    "    batch_begin = 0\n",
    "    batch_end = batch_size\n",
    "\n",
    "    # cost at the end of epoch\n",
    "    cost = 0\n",
    "    \n",
    "    # train on the batches\n",
    "    num_batches = len(x_train) // batch_size\n",
    "    while batch_end < len(x_train):\n",
    "        curr_batch = batch_end // batch_size + 1\n",
    "        print(f\"EPOCH {epoch} out of {epochs} ({curr_batch}/{num_batches})  \", end = '\\r')\n",
    "        x_batch = x_train[batch_begin : batch_end]\n",
    "        y_batch = y_train[batch_begin : batch_end]\n",
    "        \n",
    "        batch_cost = net.train_batch(x_batch, y_batch)\n",
    "        cost += batch_cost\n",
    "        \n",
    "        batch_begin += batch_size\n",
    "        batch_end = min(batch_end + batch_size, len(x_train))\n",
    "\n",
    "    costs.append(cost / num_batches)\n",
    "\n",
    "print(\"\\nTraining Done!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2bc1de01-d2c6-4161-8acc-41e9afaf4988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n",
      "    Accuracy:  87.830%\n",
      "    Cost:  0.22825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc18e200810>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxIAAAHmCAYAAADnWLjtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACR60lEQVR4nOzdd3gU1cLH8d/Mbjab3iukElroxQYiYMOGIldBRVBQUVS8FiyIXvUqlquAiooNRLgWEAuiYBcrYIFAIJSEVEjvPZvdmfcP3uwlJCEJJ9md2fw+z3Of93XYTc75zmTJYXZmJVVVVRAREREREXWC7OwBEBERERGR/nAhQUREREREncaFBBERERERdRoXEkRERERE1GlcSBARERERUadxIUFERERERJ3GhQQREREREXUaFxIALBYLsrOzYbFYnD0UIiIiIiJd4EICQH5+PhYvXoz8/HynfH9FUZCbmwtFUZzy/V0BG4pjQzHsJ44NxbGhODYUx4bi9NKQCwkNUFUVeXl54IeMnzo2FMeGYthPHBuKY0NxbCiODcXppSEXEkRERERE1GlcSBARERERUadxIaEBkiQhJiYGkiQ5eyi6xYbi2FAM+4ljQ3FsKI4NxbGhOL00NDp7AATIsozg4GBnD0PX2FAcG4phP3FsKI4NxbGhODYUp5eGPCOhATabDfv27YPNZnP2UHSLDcWxoRj2E8eG4thQHBuKY0NxemnIhYRG1NfXO3sIuseG4thQDPuJY0NxbCiODcWxoTg9NORCgoiIiIiIOo0LCSIiIiIi6jQuJDRAlmUkJCRAlrk7ThUbimNDMewnjg3FsaE4NhTHhuL00pB3bdIASZLg5+fn7GHoGhuKY0Mx7CeODcWxoTg2FMeG4vTSUNvLnB7CZrNh165dmr8yX8vYUBwbimE/cWwojg3FsaE4NhSnl4ZcSGiEoijOHoLusaE4NhTDfuLYUBwbimNDcWwoTg8NuZAgIiIiIqJO40KCiIiIiIg6TXMXW2dkZGDx4sWorKyExWLBiBEjsGDBAnh5eZ30eUVFRXjppZdw6NAhAMfeW3brrbfiwgsvdMSwhciyjMTERM1fma9lbCiODcWwnzg2FMeG4thQHBuK00tDTY2urKwMM2fOxOjRo7F+/Xps2LABWVlZWLBgwUmfV1tbi+nTp6OkpATvvfce1q9fj4ULF+Kee+7B1q1bHTN4QSaTydlD0D02FMeGYthPHBuKY0NxbCiODcXpoaGmFhJr165FXV0d5syZAwAwGo2YN28efvjhB+zcubPN533zzTc4evQobrrpJri5uQEARo8ejdNOOw0vvviiI4Yu5NWNOXjm3d2ob7A6eyi6pSgKkpKSdHFhklaxoRj2E8eG4thQHBuKY0NxemmoqYXE1q1bkZiY2GwFNmzYMMiyfNIzC4WFhQCA0NDQZtvDw8Oxf/9+lJWVdct4u4KiqNixvwq/pcp48O10FFVYnD0kIiIiIqJ2aeoaiaysLEyYMKHZNpPJhICAAGRmZrb5vNjYWADAkSNHEB0dbd+em5tr/78BAQHtfn+bzWa/X68kSZBlGYqiQFVV+2Oatp94X9+2tsuyDEmSWt0OAKqq4Mkbo/HY6jSkHq3DXcsP4Z6rotC3lxk+HgZIkgQAMBgMUFW1xcrUYDC0GGNb2x01pxPH2Nb2rp4TgBb7T+9zcuR+avpeJz5ez3Ny5H5qGpOiKDAYDC4xp/bG3tVzanp80/91hTk5ej81OX6cep+To/dT03OP/7nW+5yaxuio/dTe66Ee59Te9q6eU2uvh46ek8FgQHs0tZCora1t9f1gJpMJNTU1bT5vwoQJ6Nu3L1599VUMHjwYvr6++O6777Br1y4ALX8xasuhQ4dQWloKAAgKCkJsbCyys7NRUlJif0xERAQiIyORnp6OyspK+/aYmBgEBwfjwIEDqK+vt29PSEiAn58f9uzZ0+xgaTrzkpSUBAC4dSLwyV8y9h214rF3MwAARlmFnyfg6wHE9gqEj1mBZClDkLeKYB8g2N8dQwYPRmlpKbKysuxf29fXF3379kV+fj7y8vLs2x09pybDhw+HxWJBSkqKfZssyxgxYgQqKyuRlpZm3242mzFo0KBOz+nIkSMAgOTkZJeZk6P305AhQ5o1dIU5OWM/FRQUoHfv3i41J0fvp+TkZJebE+CY/RQWFmZv6CpzcsZ+AoCqqiqkp6e7zJycsZ/KysoQGhrqUnNy9H5KTk522pxGjRqF9kjqiUsbJxo1ahQmTJiAJUuWNNs+ZswYjB49Gi+//HKbzy0rK8OKFSuwZ88e+5XuISEhWLp0Kb777jtERUW1+dzs7GwsXrwYDz30kP2MhiNXqE2raUmS8MWOMvySXI7iikaUVDXiZG+NkyTAy90ALw8D/LwMCPFzQ6i/CSH+bggPNCPY14gQPyN8PI0On1NHtnflvyQ0nU1qGpsrzMnR+0mSJPu24/9lU89zcuR+avp6BoOBZyQEzkgoimJ/vivMydH7SZIkNDY22sfrCnNy9H5SVdXesiNj18OcmsboqP3U3uuhHufU3vbuOCNx4ushz0i0IyYmxn69QxOLxYKysjL725faEhAQgIcffrjZtmXLlsHX1xe9e/fu0PdvOuCP17SjW3tsV21XVRWNjY0wm8248uxQXHn2sWs9bIqKimoriioaUVxpQVF5I/JKGnC0uAFHSxpQXm1Fdb0N1fU2FJQBh47Utfo9/L2NiAs3Iy7cAxFB7gjydUOgjxGh/iYE+Lh1y5w6ur3ph+NEbXU/2XaLxQI3N7cWvwSLjrGz27tyTt01xta2H38cHt8Q0O+cTra9q+fU1K/phg+uMKfuGGN725saHv9LsN7ndKLunFPTLyAnvhZ2doyd3e5K+0lVVdTX18NsNrc6Hj3OqYmj9pMjXw9d6dhr0jSnE18PnTWnk9HUQmL8+PFYs2YNLBaL/S1OTaeIxo8ff9Ln/vrrrzj77LObbdu+fTsuu+yyFi+mWqMoClJSUjB8+PBmO9EgSwj0dUOgrxv6w7PV5zZaFdTU21BWbUVRuQWFZY0orLCgsMyCoopG5Jc2oLTKil1p1diVVt3i+ccWGR6IjzAjPsID8REe6B1ihtGg7WYnaqshdRwbimE/cWwojg3FsaE4NhSnl4aaWkjMmjULH330EVavXo25c+fCarVixYoVmDhxYrP3aS1cuBB79+7Fhg0b4O7uDgB48MEH8eyzz2LcuHEAgI8++ghFRUWYP3++U+biKG5GGf7eMvy93RAX7tHqY6pqrcjIr0dGfh0Kyy0orbSirKoRuSUNKKpoxK60KuxKq7I/3miQkBDpgcRYLyRGeyExxqvZmQsiIiIiIk0tJAICArBmzRosXrwY33//PRoaGjB8+HDcf//9zR7X0NCA+vr6Zu/7Ovfcc/HYY48hNDQUkiQhNjYWH3zwAQIDAx09Dc3x8TRiaLw3hsZ7t/izqlor0vPqkJ5Xh4z8ehzOrUN2YT0O5NTiQE4tPkERACA80IRBMV4YGHNsYREdaoZB1tdZCyIiIiLqOppaSABAfHw8Vq5cedLHLF26tMW2J598sruG5BBtvY+tu/l4GjGsjw+G9fGxb7NYFRw+WoeU7BqkZB37X36pBfmlFny/69hncni6yxgY/b+FRf8oT3i6O/fUm7MauhI2FMN+4thQHBuKY0NxbChODw01ddcmZ2m6a9OiRYuafQ4FHaOqKvJKLdj//4uKlOwaZBXU4/gjR5aAAdFeOK2/L04f4Iu48JYX7BIRERGR69DcGYmeSFVVVFZWwtfXV5O/fEuShMggd0QGueO8kcfeKlZTb8OB485YHMiptf//736ThyBfN/uiYngfb3h089kKrTfUAzYUw37i2FAcG4pjQ3FsKE4vDbmQ0ABFUZCWlqb5K/OP52U2YFQ/X4zq5wvg2N2j9mbW4K+DlfjjYCWOFDXgqz9L8NWfJZBlIC7cAwOiPDEw2gvD+ngj2K/lBw+K0GNDrWFDMewnjg3FsaE4NhTHhuL00pALCeoSbkYZIxJ8MCLBB7dc2gu5JQ3488CxRcW+zBoczq3D4dw6fLnj2CcsxoSZMaqvD0b29cHgOG+4u2n/fYBERERE9D9cSFC3iAxyxxVjQ3DF2BDYbCoyC+pwIKcWezNqsCutClkF9cgqqMcnvxbBzShhcKw3Rvb1wai+Pojl9RVEREREmseFhEaYzWZnD6HbGAwS+kR6ok+kJy49IxiKoiI9vw47U6uwM7UK+zJr7J9lsXILEOBjxOn9fXH5mBDER7T+2RitceWGjsKGYthPHBuKY0NxbCiODcXpoSHv2gTetcnZ6i027Emvwc7USuxMq0JOYYP9z0Yk+GDK2BD0j/KEr6eBZyqIiIiINIJnJDRAURSUlpYiMDBQF/cM7mpmkwGnDzh2hycAKCy34Os/S/DF9uJmn7rt6S4jMsgdQ/t44+zB/ujf2xPy/38oXk9v2BXYUAz7iWNDcWwojg3FsaE4vTTU7sh6EFVVkZWVBZ4cOibU34SZF0Tg3QcH4Y4remNovDeC/dxQ26AgLbcOn/xShHtXpOKG/6Rg7bd5qGuwsWEXYEMx7CeODcWxoTg2FMeG4vTSkGckSLPMJhmXnRmMy84MBgA0NCrIKazH9v0V+HVvBbIK6vH+DwXY8mcJbrggHIF81xMRERGRw3AhQbrh7iYjoZcnEnp54vrzI3A4txarvsrDztQqvPjJEUQGyHggtA79o72dPVQiIiIil8e3NmmEr6+vs4egO30iPfHU7Hg8cUM8ege7I7dMwr2vp+GtL4+irsHm7OHpEo9DMewnjg3FsaE4NhTHhuL00JB3bQLv2uQKGq0KNvxciPd/KIDVpiLU3w3/GBeK80YGwsus3U+EJCIiItIrnpHQAEVRkJubC0VRnD0U3TLIwPgBCl6d3xdD4rxQWN6IFZuOYuYz+/DaxiPYlVbFsxTt4HEohv3EsaE4NhTHhuLYUJxeGnIhoQGqqiIvL0/zV+ZrWVPDXsHueO6WBDw1Ox5nDvRFQ6OCTduL8fDKw7jq38mYv/wg3vsuH1W1VmcPWXN4HIphP3FsKI4NxbGhODYUp5eGvNiaXI4kSRjVzxej+vmioKwB3/1dhr2Z1TiQU4u03Dqk5dbh418LccWYEEw9OwQ+nvwxICIiIuos/gZFLi0swB0zzg8HANhsKg4drcUnvxTi170V+PDHAnz+exFuuDACl54ZDIPM+8cSERERdRTf2qQBkiQhKCgIksRfZE9VRxoaDBIGRnth0Yw4rPhnf4wb4o/aBgUrNh3F3a8ewsGcGgeOWHt4HIphP3FsKI4NxbGhODYUp5eGvGsTeNemnmxPehVe2XgEOYUNkCTgH+NCMfOCcJiMXGMTERERnQx/W9IARVGQmZmp+SvztexUGw6N98Gr8/vjxkkRMBokbPi5EPe8lorswvpuGql28TgUw37i2FAcG4pjQ3FsKE4vDbmQ0ABVVVFSUqL5K/O1TKShm1HG9AlheOmOfogNNyM9rw7zlx/E13+VdMNItYvHoRj2E8eG4thQHBuKY0NxemnIhQTR/4sL98BLt/fD1LNDYLGqePHjHKzckgubou0fYiIiIiJn4EKC6DgmNxm3XNoLi2bEwt3t2FudFr+XwQ+zIyIiIjoBFxIaIEkSIiIiNH9lvpZ1dcOzB/vjP3P7ItDHiG0plbhz+UFs3V0GxYXPTvA4FMN+4thQHBuKY0NxbChOLw151ybwrk3UtqIKCxb/NxMHj9QCAOLCzbhxUgROH+Dn5JERERERORfPSGiAzWZDamoqbDa+feZUdVfDED8Tls7ri4evi0VUiDsy8uvx2LsZePaDTFTWWLv0ezkbj0Mx7CeODcWxoTg2FMeG4vTSkJ9srRGVlZXOHoLudVdDWZYwbog/xiT64ftdpXh7cy5+2lOOPenVmH9lFM5KdJ2zEzwOxbCfODYUx4bi2FAcG4rTQ0OekSDqIINBwoWjg/D6PQNw5kBflFVb8e+1GfhyR7Gzh0ZERETkcFxIEHVSoI8b/jUzDnf/IwqSBLy+6Sj2Z9c4e1hEREREDsWFhAZIkoSYmBjNX5mvZY5uKEkSJo0Owoxzw2G1qVj8XibKqhod8r27C49DMewnjg3FsaE4NhTHhuL00pALCQ2QZRnBwcGQZe6OU+WshteeG4bT+vuipLIRz36YBZtNvzdB43Eohv3EsaE4NhTHhuLYUJxeGmp7dD2EzWbDvn37NH9lvpY5q6EsS7h/WjTCA0zYk16NVV/lOvT7dyUeh2LYTxwbimNDcWwojg3F6aUhFxIaUV9f7+wh6J6zGvp4GvHI9cc+CfuTX4vwzV8lThlHV+BxKIb9xLGhODYUx4bi2FCcHhpyIUHUBfpEeuK+q2MAAMs/O4J9mdVOHhERERFR9+JCgqiLjBvijxnnhcFqU/HUe5koKLM4e0hERERE3YYLCQ2QZRkJCQmav6BGy7TS8Lpzw3H2YD+UV1vx0Ftp2JVW5dTxdIZWGuoV+4ljQ3FsKI4NxbGhOL001PboeghJkuDn56f5W3xpmVYayrKE+66OxpA4L+SXWfDwysN4YX0WyqutTh1XR2iloV6xnzg2FMeG4thQHBuK00tDLiQ0wGazYdeuXZq/Ml/LtNTQbDLg2ZsTcOeU3vB0l/H9rjLc9uIBpB2tdfbQTkpLDfWI/cSxoTg2FMeG4thQnF4aciGhEYqiOHsIuqelhrIs4dIzgvHmvQNxxkBfVNRY8dDbaTig8U/A1lJDPWI/cWwojg3FsaE4NhSnh4ZcSBB1oyBfNzx6fRwmjQ5ETb2Ch1cext4M3tGJiIiI9I8LCaJuZpAl3HVlFCafGYw6i4JH3jmMgznafpsTERERUXu4kNAAWZaRmJio+SvztUzrDWVZwrzLe2HK2BA0NKp4fn0W6i3aOmWp9YZax37i2FAcG4pjQ3FsKE4vDbU9uh7EZDI5ewi6p/WGkiTh5osjMSDKE0eLG7D661xnD6kFrTfUOvYTx4bi2FAcG4pjQ3F6aMiFhAYoioKkpCRdXFSjVXppaDAcuz2su5uEjb8XY/dh7XzOhF4aahX7iWNDcWwojg3FsaE4vTTkQoLIwXqHmDF7UiQAYOmGbNTUa/vWbkRERESt4UKCyAkmnxWMYfHeKCxvxKsbj0BVVWcPiYiIiKhTjM4ewIkyMjKwePFiVFZWwmKxYMSIEViwYAG8vLxO+rzs7GwsWbIEOTk58PLyQm1tLa666ipce+21Dho5UcfJsoR7rorG/OUH8WNSGfr19sSUsSHOHhYRERFRh0mqhv4ptKysDJMnT8b111+P2267DVarFXPnzoW7uztWrFhx0udeeOGFiI+PxyuvvAKj0Yjs7GxcccUVeOSRR/CPf/zjpM/Nzs7G4sWLsWjRIkRHR3fllDpEVVUoigJZljX/UehapdeGO1Or8Og7hwEJeHpOHwzr4+O0sei1oVawnzg2FMeG4thQHBuK00tDTb21ae3atairq8OcOXMAAEajEfPmzcMPP/yAnTt3tvm88vJyZGVlYdy4cTAaj51kiY6ORlxcHH744QeHjF2UxWJx9hB0T48NR/b1weyLIqEowNMfZKKgzLlz0GNDLWE/cWwojg3FsaE4NhSnh4aaemvT1q1bkZiY2Ox2V8OGDYMsy9i6dStGjhzZ6vP8/f0xbtw4bNmyBZdffjl8fHyQlJSE1NRUDB06tMPf32azwWY7duGrJEmQZRmKojR7/3rT9qbHtbe9aSXZ2nbg2FX5NpsNKSkpGDJkCNzc3Ozbj2cwGOyr0xO3nzjGtrY7ck4d2d6Vc7JarfaGBoNBV3O6cmwQUo/U4OfkCjzyzmE8PisWvYI9HL6fVFVt1lBkTj3p2Gva3vRzPHToULi5ubnEnNobe1fPqbGxsdkx6ApzcvR+au3nWO9zcvR+avpZHjZsWIt/CdbrnJrG6Kj91N7roR7n1N72rp5Ta6+Hjp7T8b8LtEVTC4msrCxMmDCh2TaTyYSAgABkZmae9LkrVqzAE088gXPOOQfh4eHIyMjAqFGjcOedd3b4+x86dAilpaUAgKCgIMTGxiI7OxslJSX2x0RERCAyMhLp6emorKy0b4+JiUFwcDAOHDiA+vp6+/aEhAT4+flhz549zQ6WpgVTUlKSfVtycjKGDx8Oi8WClJQU+3ZZljFixAhUVlYiLS3Nvt1sNmPQoEEoLS1FVlaWfbuvry/69u2L/Px85OXl2bc7Y04AHDKnI0eO2BvqbU4FBQWY0KcUGUdl5BQ14J+vHMKjM+PhZyhx6H4aMmRIs4Yic+pJx96JcyooKEDv3r1dak6O3k/JyckuNyfAMfspLCzM3tBV5uSM/QQAVVVVSE9Pd5k5OWM/lZWVITQ01KXm5Oj9lJyc7LQ5jRo1Cu3R1DUSAwcOxJQpU/DMM8802z5hwgT06dMHK1eubPV5qqpi3rx5KCkpwRtvvIHAwEAcPHgQ3377LW666SZ4eHic9Ps2XSPx0EMP2a+RcPQZieTkZJ6REJhTY2Mj9uzZo8szEk3bGxoVLP/sKLbuLocsA7dd1guXnB7Y7ti78ozE7t27eUZC4IxEcnIyz0gIzKmxsdH+WsgzEqc2J1VVkZSUxDMSgmckkpOTeUZC8IzEyV4P9Tin9rZ3xxmJE18PeUaiHZ6enq2+H8xisZz0rk0//vgjfvzxR7z99tsIDDz2i1f//v3x1ltv4Z577sHrr7/eoe/ftKOO17SjW3tsV26XZdn+C3Bbj5ckqdXtbY2xs9u7ek4d2d6Vc2pqeOIvwaJj7Oz2U52Tp8GAB6bHIC7CA6u/zsNrnx+Fu5uMC0cHOWTsNput1YYic+rodj3tp5NtbzoOu2qMnd3uCvvpxGPQFeZ0ou6c08l+jvU6p64aY2e2N/1S6EpzAhy7nxz1eujK++nEn2VnzelkNHWxdUxMDAoLC5tts1gsKCsrQ2xsbJvPazr1eOIdl2JiYvDjjz+iurq6y8falQwGA0aMGHFKO5COcZWGkiRh2vgwPDAtBgDw8qc52JPumE+/dpWGzsJ+4thQHBuKY0NxbChOLw01tZAYP348UlJSmp2VaHqv2fjx49t8XkREBAC0WITk5+fDzc2t2cXbWqSqKioqKvihZAJcreGE4QG44cII2BTgyf9m4khRfftPEuRqDR2N/cSxoTg2FMeG4thQnF4aamohMWvWLHh4eGD16tUAAKvVihUrVmDixInNLvhYuHAhJk+ejIaGBgDHFiC9evXCm2++aV+EpKWlYfPmzZg0aZLmFxKKoiAtLa3F++So41yx4fQJoTh/ZACq62x4/N0MVNVau/X7uWJDR2I/cWwojg3FsaE4NhSnl4aaukYiICAAa9asweLFi/H999+joaEBw4cPx/3339/scQ0NDaivr7ev0ry9vfHuu+9i2bJlmD59OsxmM6qrqzFr1izceuutzpgKkTBJkjD/yijkl1mwN6MGz3yQiSdv7AODQbsfTENEREQ9h6YWEgAQHx/f5t2ZmixdurTFtqioqFa3E+mZySjjkRlx+Oerh7ArrRort+Ri7mW9nD0sIiIiIm29taknM5vNzh6C7rlqQz8vI/41Mw7ubjI+/a0I3+0s7bbv5aoNHYX9xLGhODYUx4bi2FCcHhpq6nMknKXpcyQWLVrU4s5PRFrxS3I5nn4/E25GCU/f1AeDY72dPSQiIiLqwXhGQgMURUFxcbHmL6jRsp7QcNwQf0yfEIZGq4qFbx/Gx78UQlG67t8BekLD7sR+4thQHBuKY0NxbChOLw25kNAAVVWRlZWl+Vt8aVlPaTjrgnDMOC8MiqLi7c25eOzddJRXN3bJ1+4pDbsL+4ljQ3FsKI4NxbGhOL005EKCSEdkWcL150fgmZsTEOTrhr8OVWH28/vx2udHcLS4wdnDIyIioh6ECwkiHRoa743X7uqPCcMCYGlUsGlbMW5Zuh9PrElHWm6ts4dHREREPYDmbv/aU/n6+jp7CLrX0xr6ehnx4DUxuHFSOD7fVoyv/ijB9v2V2L6/EmcP9sP150cgJqxzd3zoaQ27GvuJY0NxbCiODcWxoTg9NORdm8C7NpFrqKm3YeNvRfj4l0LUNiiQJeDSM4Ix68JweHvw3wyIiIioa/GtTRqgKApyc3M1f2W+lrEh4GU24LrzwrH6gURMnxAKo0HCpu3FuGXpAXy/s7TdC7bYUAz7iWNDcWwojg3FsaE4vTTkQkIDVFVFXl6e5q/M1zI2/B8fTyNunBSJ1+8ZgNMH+KK82ooXPsrGA2+lITO/rs3nsaEY9hPHhuLYUBwbimNDcXppyIUEkYuKCHTHEzfE47GZcQj1d8PejBrcufwg3t58FHUNNmcPj4iIiHSOCwkiF3dmoh/euGcgpk8IgyRJ+PiXItz9Wiqq66zOHhoRERHpGBcSGiBJEoKCgiBJkrOHoltseHJmk4wbJ0VgxT/7o28vD2QX1mPxe5mw2v53ypQNxbCfODYUx4bi2FAcG4rTS0PetQm8axP1LOXVjbj7tVQUlFlw0WmBuOvKKM2/UBEREZH28IyEBiiKgszMTM1fma9lbNhx/t5ueOKGeHi6y/jqz1J8/EsRADYUxX7i2FAcG4pjQ3FsKE4vDbmQ0ABVVVFSUqL5K/O1jA07JybMjEUz4iDLwKqvcvHTnjI2FMR+4thQHBuKY0NxbChOLw25kCDqoUb29cH8KVFQVeCF9dlISqt29pCIiIhIR7iQIOrBLjotCLMuCIfVpmLx+1k4WubsEREREZFecCGhAZIkISIighe8CmDDU3fNxDBMPjMYdRYF//3diIKyRmcPSZd4DIpjQ3FsKI4NxbGhOL005EJCA2RZRmRkJGSZu+NUseGpkyQJt07uhXFD/FFVp2LJhhzYFG2/J1OLeAyKY0NxbCiODcWxoTi9NNT26HoIm82G1NRU2Gz8tOFTxYZiDLKEu6ZEIsRXQkpWDT7+udDZQ9IdHoPi2FAcG4pjQ3FsKE4vDbmQ0IjKykpnD0H32FCMh7sBU0dZIUvA2u/ykZZb6+wh6Q6PQXFsKI4NxbGhODYUp4eGXEgQkV1UEDBtfCisNhXPr8tGQ6O2719NREREzsOFBBE1c83EUPTt5YHswnq88cVRzd/DmoiIiJyDCwkNkCQJMTExmr8yX8vYUFxTQzejjAemx8DTXcaWP0rsn3xNJ8djUBwbimNDcWwojg3F6aUhFxIaIMsygoODNX9lvpaxobjjG/YOMePh62Ihy8DKLbn4dW+5s4eneTwGxbGhODYUx4bi2FCcXhpqe3Q9hM1mw759+zR/Zb6WsaG4ExuO6ueLO6+IAgA8vy4LB7JrnDk8zeMxKI4NxbGhODYUx4bi9NKQCwmNqK+vd/YQdI8NxZ3Y8OLTg3D1+FBYrCqeei8TtQ3afkFzNh6D4thQHBuKY0NxbChODw25kCCik7rxwggM7+ONkspGfPBDgbOHQ0RERBrBhQQRnZQsS7j98t4wGiR8+mshsgq0/y8kRERE1P24kNAAWZaRkJCg+QtqtIwNxZ2sYVSoGVPPDoFNAVZ8foS3hG0Fj0FxbCiODcWxoTg2FKeXhtoeXQ8hSRL8/Pw0f4svLWNDce01vPbcMIT4uWF3ejV+2lPu2MHpAI9BcWwojg3FsaE4NhSnl4ZcSGiAzWbDrl27NH9lvpaxobj2GppNBsy9rBcA4K0vjyKvtMGRw9M8HoPi2FAcG4pjQ3FsKE4vDbmQ0AhFUZw9BN1jQ3HtNRw7yA+nD/BFaZUVd792CClZvCXs8XgMimNDcWwojg3FsaE4PTTkQoKIOkySJCy6LhbnDPVHZY0ND72dhq27y5w9LCIiInICLiSIqFNMbjIenB6DayeGodGq4rkPs/D1XyXOHhYRERE5GBcSGiDLMhITEzV/Zb6WsaG4zjSUZQmzLozA3f849snXyz/Nwa60qu4eoqbxGBTHhuLYUBwbimNDcXppqO3R9SAmk8nZQ9A9NhTX2YaTRgdh9kURsCnA4vcyevxnTPAYFMeG4thQHBuKY0NxemjIhYQGKIqCpKQkXVxUo1VsKO5UG159TigmjQ5ETb2Cx95NR1lVYzeNUNt4DIpjQ3FsKI4NxbGhOL005EKCiIRIkoQ7p0RheB9vFJRZ8Pz6bH5gHRERUQ/AhQQRCTMaJCyaEYtgPzfsSqvC13+VOntIRERE1M24kCCiLuHtYcQ/rzx28fVbXx5FUYXFySMiIiKi7iSpfA8CsrOzsXjxYixatAjR0dEO//6qqkJRFMiyrPmPQtcqNhTXVQ2XbsjGt3+XYnQ/H/z7xvgesz94DIpjQ3FsKI4NxbGhOL005BkJjbBY+K+3othQXFc0vOXSSAT6GPHXoSp8t7NnfVgdj0FxbCiODcWxoTg2FKeHhlxIaICiKEhJSdH8lflaxobiuqqhj4cR8///LU6vbzqCgrKGrhie5vEYFMeG4thQHBuKY0NxemlodPYATpSRkYHFixejsrISFosFI0aMwIIFC+Dl5dXmc3bs2IF7770X8fHxzbaXl5cjMzMTf/31F9zd3bt76ET0/84c6IcLRgXi279L8Z912fjPLQkwGLR7apaIiIg6T1NnJMrKyjBz5kyMHj0a69evx4YNG5CVlYUFCxa0+9xx48Zh7dq1zf43ZswYXHzxxVxEEDnBvMm9EBlkQkpWDT7cWuDs4RAREVEX09RCYu3atairq8OcOXMAAEajEfPmzcMPP/yAnTt3tvm8IUOG4J577mm2raGhAZ999hmuvfbabh1zV9H6R6DrARuK68qGHu4GPDA9FgYZeP/7fOzLrO6yr61VPAbFsaE4NhTHhuLYUJweGmpqhFu3bkViYmKzjwQfNmwYZFnG1q1b23yep6cnwsLCmm3bsmULIiIiMGLEiO4abpcxGAwYMWIEDAaDs4eiW2worjsa9o/yxKwLIqCowH/WZaGqztplX1treAyKY0NxbCiODcWxoTi9NNTUNRJZWVmYMGFCs20mkwkBAQHIzMzs1Ndat25dp89G2Gw22Gw2AMc+rVeWZSiK0uxTepu2Nz2uve1Nt+1qbTsA+9evqqqCj4+P/YA58eIag8FgvxXYidtPHGNb2x05p45s78o52Ww2VFZWwsfHB5IkucScHL2fJElCZWUlvL29m91qTnROU8YGYWdqFXanV+OF9Vl45LoYyLLkkDk5cj81/Rz7+vrCYDC4xJzaG3tXz8lms9lfCyVJcok5OXo/SZKE8vJye0NXmJOj95OqqqipqYGPj0+Hxq6HOTWN0VH7qb3XQz3Oqb3tXT2n1l4PHT2njixiNLWQqK2tbXY2oonJZEJNTU2Hv05qaipSU1MxefLkTn3/Q4cOobT02CfyBgUFITY2FtnZ2SgpKbE/JiIiApGRkUhPT0dlZaV9e0xMDIKDg3HgwAHU19fbtyckJMDPzw979uxpdrA0nXlJSkpqNobhw4fDYrEgJSXFvk2WZYwYMQKVlZVIS0uzbzebzRg0aBBKS0uRlZVl3+7r64u+ffsiPz8feXl59u2uPqemfedKc3LkfhoyZAgOHz7cbFtXzenasQHIKTLijwNVWL5+D8YPUB0yJ2fsp7CwMPTu3dul5uSK+8lV5xQWFob09PRmY9H7nJyxnxRFQXx8fLOWep+TM/ZTVFQUQkNDXWpOrrif2prTqFGj0B5NfSDdqFGjMGHCBCxZsqTZ9jFjxmD06NF4+eWXO/R1nnzySUiShEceeaRDj2/6QLqHHnrI/oF0jlyh2mw2JCcnY8iQIXBzc7NvP54rr7q7Yk6NjY3Ys2cPhgwZAoPB4BJzcvR+UlUVu3fvtjfs6jmlZNXiwbfTABV4anYchsZ7u9R+avo5Hjp0KNzc3FxiTu2Nvavn1NjYaH8tNBgMLjEnR+8nVVWRlJTU7OdY73Ny9H5q+lkeNmxYiw8C0+ucmsboqP3U3uuhHufU3vaunlNrr4c8I9GOmJgYFBYWNttmsVhQVlaG2NjYDn2Nuro6fP755/jwww87/f2bdtTxmnZ0a4/t6u1NvwC39fimU1snamuMnd3eHXNqb3tXz+nEfegKc+qOMba2vekFpLWfg66Y0+A4b8yeFImVW3Lxn/U5eHJ2PBIiPbtk7Keyvbv20/HHougYO7tdr8feiduPPwZdZU7H6845neznWK9z6qoxdnY75yQ+J0e8Hrr6fjr+Z9lZczoZTV1sPX78eKSkpDT7JL+mU0Tjx4/v0Nf48ssvMWDAAPTp06e7htktzGazs4ege2worrsb/mNcCMYM8kN5tRV3vXIIL6zPQlGF9j+5s6N4DIpjQ3FsKI4NxbGhOD001NRbm8rKyjB58mTMmjULc+fOhdVqxa233go3Nze8/vrr9sctXLgQe/fuxYYNG1p8RsTVV1+N2bNn45JLLunw9216a9OiRYvsb20iou5haVSw4edCfPRzIeotCkxGCbde1guXnBHs7KERERFRJ2jqjERAQADWrFmDHTt2YPr06bjqqqsQFRXV4pqJhoYG1NfXt3if2P79+5GXl4cLLrjAkcMWpigKiouLW7xPjjqODcU5qqHJTcZ154Vj5YKBuOT0IFhtKpZ/dgQbfi5s/8kaxmNQHBuKY0NxbCiODcXppaGmrpEAgPj4eKxcufKkj1m6dGmr2wcOHIhff/21O4bVrVRVRVZWFgICApw9FN1iQ3GObhjo44b5V0ZhaB9v/GddFlZuyUWjVcG154Y75Pt3NR6D4thQHBuKY0NxbChOLw01dUaCiHqe8UMD8MiMOBgNEtZ8m4+VW3JhUzTzjksiIiJqAxcSROR0ZyX64bGZcTAZJWz4uRD/Wp2OyhrX/RRsIiIiV8CFhEb4+vo6ewi6x4binNlwdH9f/GduX4T4uWFnahXmv3IQqUdrnTaeU8FjUBwbimNDcWwojg3F6aGhpu7a5Cy8axORdlTUWPHch5nYlVYNk1HCo9fHYXR/7b+YEhER9TQ8I6EBiqIgNzdX81fmaxkbitNKQz8vI56c3Qf/GBcCi1XFE2szsH1/hVPH1BFa6adnbCiODcWxoTg2FKeXhlxIaICqqsjLy2txO1vqODYUp6WGBlnCzZf0wszzw2G1qXjqvxn4bW+5s4d1Ulrqp1dsKI4NxbGhODYUp5eGXEgQkWZdd144Zl8UAZsCPP1BJranaP/MBBERUU/BhQQRadq08WG45ZJIKArw7IdZursAm4iIyFVxIaEBkiQhKCgIkiQ5eyi6xYbitNxw6rhQTD07BA2NCh57Nx2F5RZnD6kFLffTCzYUx4bi2FAcG4rTS0MuJDRAlmXExsZClrk7ThUbitN6wzkXR+KsRD+UVVnx2LvpqKm3OXtIzWi9nx6woTg2FMeG4thQnF4aant0PYSiKMjMzNT8lflaxobitN7QIEt4YHo0+vbyQGZ+PV7beMTZQ2pG6/30gA3FsaE4NhTHhuL00pALCQ1QVRUlJSWavzJfy9hQnB4amk0GPD4rHp7uMn5IKsOB7BpnD8lOD/20jg3FsaE4NhTHhuL00pALCSLSlUBfN1x7bjgA4M0vj2r+RZaIiMhVcSFBRLpz+ZhghAeasD+7Fj/tKXf2cIiIiHokLiQ0QJIkREREaP7KfC1jQ3F6amgyyrjp4kgAwKotuWhodP57SPXUT6vYUBwbimNDcWwoTi8NuZDQAFmWERkZqfkr87WMDcXpreHYQX4YHOeFoopGfPJrobOHo7t+WsSG4thQHBuKY0Nxemmo7dH1EDabDampqbDZtHU7Sz1hQ3F6ayhJEm69tBckCVj3YwGOFNU7dTx666dFbCiODcWxoTg2FKeXhlxIaERlZaWzh6B7bChObw0TenniijEhaGhU8cJH2bDZnHvhtd76aREbimNDcWwojg3F6aEhFxJEpGs3TopAVIg7DubUYv3PBc4eDhERUY/BhQQR6Zq7m4z7ro6GLAPvf1+Aw7m1zh4SERFRj8CFhAZIkoSYmBjNX5mvZWwoTs8N+0d5Yfr4MFhtKl5Yn43aBse/p1TP/bSCDcWxoTg2FMeG4vTSkAsJDZBlGcHBwZq/Ml/L2FCc3htee24YEiI9kFlQj8feTUe9xbGLCb330wI2FMeG4thQHBuK00tDbY+uh7DZbNi3b5/mr8zXMjYUp/eGbkYZj98Qj8ggE/Zm1OCJNRkO/XwJvffTAjYUx4bi2FAcG4rTS0MuJDSivt65t650BWwoTu8Ng3zd8OzNCQgPMCHpcDWe+m8GLA5cTOi9nxawoTg2FMeG4thQnB4aciFBRC4lxN+EZ2/pgxA/N/x1qAqPvHMYVXVWZw+LiIjI5XAhQUQuJyzAHc/NTUBkkAnJGTVY8HoaCsstzh4WERGRS+FCQgNkWUZCQoLmL6jRMjYU52oNIwLdseS2fugf5Ynswnrc89ohbN1dhpr67nm/qav1cwY2FMeG4thQHBuK00tDSVVV534UrAZkZ2dj8eLFWLRoEaKjo509HCLqQvUWBc99mInt+499QqhBBgbHeePKsSE4Y6Cfk0dHRESkX9pe5vQQNpsNu3bt0vyV+VrGhuJctaHZJOOR6+Nwzz+icPoAXxhkCbsPV+PJ/2bg0JGu+/A6V+3nSGwojg3FsaE4NhSnl4ZcSGiEojjuzjKuig3FuWpDgyzhwtFBeOKGeKx7dAiuOicUNgV47sNM1HXhh9e5aj9HYkNxbCiODcWxoTg9NORCgoh6FLNJxo0XRiAxxgu5JRas2HTU2UMiIiLSJS4kiKjHMRgkPDA9Bl5mGd/+XYqtu8ucPSQiIiLd4UJCA2RZRmJiouavzNcyNhTX0xqGBZgw/8ooAMDLn+Rg4+9FsNlO/d4TPa1fd2BDcWwojg3FsaE4vTTU9uh6EJPJ5Owh6B4biutpDccPDcCVY0NQZ1Hw+qajuHP5QSQdrjrlr9fT+nUHNhTHhuLYUBwbitNDQy4kNEBRFCQlJeniohqtYkNxPbXh3Mt64blbEhAbbkZmQT0Wvn0YG34u7PTX6an9uhIbimNDcWwojg3F6aUhFxJE1OMNjffGK3f2x22Te0GWgZVbcvHFtmJnD4uIiEjTuJAgIsKxC7CvGBOC+6fFQJKAVz8/gm//LnX2sIiIiDSLCwkiouNMGBaAu6ceuwj7xY+z8dWfJVDVU78Im4iIyFVJKv+GRHZ2NhYvXoxFixYhOjra4d9fVVUoigJZliFJksO/vytgQ3Fs2NznvxfZP2PinCH+uPPK3vDxMLb5ePYTx4bi2FAcG4pjQ3F6acgzEhphsVicPQTdY0NxbPg/l48JwWMz4+DracDPyeW446WD2JladdKzE+wnjg3FsaE4NhTHhuL00JALCQ1QFAUpKSmavzJfy9hQHBu2dGaiH1775wCMSPBGUUUjFq06jPvfTMPfhypbLCjYTxwbimNDcWwojg3F6aUhFxJERCcR5OuGp2b3wR1X9EaQrxv2ZdbgkXfSseCNNJRWNTp7eERERE7DhQQRUTtkWcJlZwZj1f0DMX9Kb4T6uyElqwYPvJGGonLtn3omIiLqDlxIaITWPwJdD9hQHBuenMko45IzgvHaPwdgcKwXjpY0YMEbqcgtaQDAfl2BDcWxoTg2FMeG4vTQkHdtgvPv2kRE+lNvUfDkfzOwM7UKgT5GPHFDPBJ6eTp7WERERA6j/aVOD6CqKioqKnivegFsKI4NO8dskvHYrDicleiH0ior7n09FR//lKP5C+O0jMegODYUx4bi2FCcXhpqbiGRkZGBm2++GdOmTcOUKVPwxBNPoKampkPP3blzJ2655RbccMMNuOKKK3D55Zdj3bp13TxicYqiIC0tjb+ACGBDcWzYeSajjEUzYjF9QhgarSre/qoESz7KRr2FDU8Fj0FxbCiODcWxoTi9NNTUQqKsrAwzZ87E6NGjsX79emzYsAFZWVlYsGBBu8/dtm0b7rvvPjz88MN49913sXHjRpxxxhn4888/HTByIuqpDLKEGydF4F/Xx8DspuKHpHL8e206rDZt/ysSERGRKE0tJNauXYu6ujrMmTMHAGA0GjFv3jz88MMP2LlzZ5vPU1UVjz32GG666SbExcXZt8+bNw833XRTt4+biOj0Ab64/XwFof5u2JVWjTe+OOrsIREREXUro7MHcLytW7ciMTERJpPJvm3YsGGQZRlbt27FyJEjW33enj17kJWVhTFjxjTbHhgYiMDAwA5/f5vNBpvNBgCQJAmyLENRlGbvT2va3vS49rY3fbR5a9uBY6eubDYb3N3dYbPZmm0/nsFgsH9c+onbTxxjW9sdOaeObO/qOTU1dKU5OXI/qararKErzMmR+8lmsyEi0B2PzIjGg29l4IvtxegV7IbJZwbrdk4nbu/u/XT8a6GrzMnR+wlAi59jvc/J0fvJZrPBbDbbf65dYU5NY3TUfmr6WVYUBQaDwSXm1N72rp5Ta6+Hjp6TwWBAe055IfHTTz9h/Pjxrf7ZG2+8gUOHDuGhhx5CSEhIh79mVlYWJkyY0GybyWRCQEAAMjMz23ze/v37AQAFBQV4/vnnUVZWBrPZjIsuugjTpk3r8O2zDh06hNLSUgBAUFAQYmNjkZ2djZKSEvtjIiIiEBkZifT0dFRWVtq3x8TEIDg4GAcOHEB9fb19e0JCAvz8/LBnz55mB0vTgikpKcm+LTk5GcOHD4fFYkFKSop9uyzLGDFiBCorK5GWlmbfbjabMWjQIJSWliIrK8u+3dfXF3379kV+fj7y8vLs250xJwAOmdPRo0fR0NCA5ORkl5mTM/ZTnz597A1dZU6O3k+BchUeujYGj6/JwJtf5KK+/Aj6hut7To7eT8nJyS43J8Bx+8nd3b3Zz7ErzMkZ+6miosLl5uTo/VRRUeFyc3L0fkpOTnbanEaNGoX2nPLtXyMjI5Gbm9vqn23atAnLli2Dp6cnvvjiiw5/zYEDB2LKlCl45plnmm2fMGEC+vTpg5UrV7b6vNdffx3Lli1Dv3798MYbbyAyMhL79u3DjTfeiKlTp2LhwoUn/b5Nt3996KGH7Ld/deQKVVEUlJWVISAgAEaj0b79eK686u6KOVmtVpSWliIgIMA+Pr3PydH7CQBKS0vh7+/fbPGt5zk5cj81/RwHBgbCaDTi458L8PaWPHi6y1hyax9Eh3nobk4nbu/u/WS1Wu2vhbIsu8ScnHFGori42N7QFebk6P2kKAoqKioQEBCAE+l1Tk1jdNR+OvH10BXm1N72rp5Ta6+HLnVG4mTrj8mTJ+OSSy5BVFRUp76mp6cnLJaWnxJrsVjg5eXV5vOadsb111+PyMhIAMCgQYNw1VVX4Z133sH8+fPh7e3d7vc3GAwtorV1NqOtuKe6PScnB0FBQfa/CFp7vCRJrW5va4yd3d7Vc+rI9q6akyRJ9obHfz09z8nR+8lmsyE7OxuBgYEt/kyvczrZ9u6YU9MxCABTx4XiSHEDvvqzFE/8Nwsv3d4Pvl6y7ubU1WM82XZZllv8HOt9To7eTzabrdXXws6OsbPbXW0/ZWVlISAgwKXmBDh2Px3/eugqc3LU9qZf7E/8WXbWnE6mUwuJ7Oxs+/+vKApycnJaXVA0NjZi7969nR5MTEwMCgsLm22zWCwoKytDbGxsm89rWjw0/d8m0dHRUFUVWVlZGDRoUKfHQ0R0qiRJwu2X90ZuiQV70qvx5HsZeHpOH7gZNXWPCyIiolPWqYVEbGys/V/Mm/77ZO64445ODWb8+PFYs2YNLBaL/YLrpveatXU9BgCcccYZMBgMyM/Pb7a9aVESHBzcqXEQEXUFN6OMRdfF4p4Vh7A3owavbDyCu6dGNXsdJSIi0qtOLST+9a9/QZIkqKqKpUuX4r777mv1cV5eXhg0aBAuvvjiTg1m1qxZ+Oijj7B69WrMnTsXVqsVK1aswMSJE5td8LFw4ULs3bsXGzZsgLu7O0JCQnDdddfhvffew6RJk+Dr64uCggJ8/PHHuPzyyxEWFtapcTiDr6+vs4ege2wojg3FtNbP18uIx2fF454Vh/DNX6WIDjXjH+NCnTA6feAxKI4NxbGhODYUp4eGp3yxdUJCQrOr5LtKeno6Fi9ejOrqajQ0NGD48OG4//77m10jce+99yI5ORmbNm2C2WwGcOx9oa+++iq+/fZb+Pj4wGKxYNKkSbjhhhua3U62NU0XWy9atMh+sTURUVfalVaFR945DFUF/jUzDmcO9HP2kIiIiISc8kLClTh7IaEoCvLz8xEeHt7mhTF0cmwojg3FdKTfF9uL8erGI/AwyVhyW1/ERXg4eJTaxmNQHBuKY0NxbChOLw27dGQNDQ145513sGzZMmRkZHTll3ZpqqoiLy/vpHfCopNjQ3FsKKYj/S47MxiTzwpGnUXB42vSUVrZ6MARah+PQXFsKI4NxbGhOL00POWFxDvvvAODwYDExEQAx1ZO5513Hm6++Wbcd999GDp0aLMPxCEiIuDWS3thVF8fFJY34sG307iYICIi3TrlhcTHH3+MOXPm4PfffwcAfPbZZ/j9998xefJkbNq0Ceeffz6eeuqpLhsoEZErMBgkPDwjFoNivXCkqAEPvJWG4oqWn59DRESkdae8kNi3bx9efvll+Pv7AwD++9//wsvLC2vWrMGll16KVatWYceOHV01TpcmSVKzD6OjzmNDcWwopjP9PN0N+PeN8Rgc64WjxQ148K00FHExwWOwC7ChODYUx4bi9NLwlBcSFosFHh7HLhSsra3FN998g6lTp9pvVRUQEIDGRp6y7whZlhEbG6vpi2m0jg3FsaGYzvazLybivJBbYsG9K1KRkVfXzaPUNh6D4thQHBuKY0Nxeml4yqPz8vLC4cOHAQBr165FXV0dpk2bZv/zyspKuLu7i4+wB1AUBZmZmVAUxdlD0S02FMeGYk6ln4e7AU/eGI/R/XxQXNGI+95Ixd+HKrtxlNrGY1AcG4pjQ3FsKE4vDU95ITFt2jRMnDgRV199Ne655x7ExMTgkksuAQAUFxfjn//8J4YMGdJlA3VlqqqipKRE81fmaxkbimNDMafaz2wy4PFZ8bj0jCDUNSj417vp+PrPkm4apbbxGBTHhuLYUBwbitNLw059svXxHnnkEeTm5uKzzz5DTEwMVq9eDUmSYLPZEBoaCkmSsHbt2q4cKxGRSzIYJNxxRW9EBrnj7S25ePnTHPQKdsfgOG9nD42IiKhNp7yQMJvNWLVqFVatWtVsu8Fg0PxpGCIirZEkCVPHhcKqqHjnqzw8+2EWXpnfH/7ep/wyTURE1K267AqO4uJiFBcXd9WX61EkSUJERITmr8zXMjYUx4ZiuqrfVeNCcVp/X5RUNuKF9VlQFG2f1u5KPAbFsaE4NhTHhuL00lBoIVFWVobbb78dwcHBCAsLQ1hYGIKDg3HHHXegrKysq8bo8mRZRmRkpOavzNcyNhTHhmK6qp8sS1hwdTSC/dzwd2oVPvq5sItGqH08BsWxoTg2FMeG4vTS8JRHV1JSgtNPPx2vv/46Ghsb0a9fP/Tr1w+NjY1YsWIFzjzzTJSWlnblWF2WzWZDamoqbDabs4eiW2wojg3FdGU/Xy8jFl4bC4MMrPkmDxn5PeO2sDwGxbGhODYUx4bi9NLwlBcSjz/+OLy9vfH999+joqIC+/fvx/79+1FRUYFvv/0Wnp6eeOKJJ7pyrC6tsrLn3vKxq7ChODYU05X9EmO8MG1CGBQVeHtzbpd9Xa3jMSiODcWxoTg2FKeHhqe8kPjiiy/w2WefYeLEiS3+7LzzzsOnn36KjRs3Cg2OiKgnu/qcUAT4GLEztQp/HdT+XyhERNSznPJCor6+HjExMW3+eWxsLBoaGk71yxMR9Xge7gbccEEEAOCtzbmw2XrOhddERKR9p7yQcHd3R3Z2dpt/npmZCZPJdKpfvkeRJAkxMTGavzJfy9hQHBuK6a5+548KRFy4GdmF9fj6L9f+oDoeg+LYUBwbimNDcXppeMoLicsuuwxTpkzB1q1bW/zZjz/+iKlTp+Lyyy8XGVuPIcsygoODNX9lvpaxoTg2FNNd/QyyhJsv6QUAWPtdPmrqtX3hnQgeg+LYUBwbimNDcXppeMqje+KJJ1BZWYnzzjsPvr6+9rs2+fr64vzzz0d1dTUef/zxLhyq67LZbNi3b5/mr8zXMjYUx4ZiurPfyL4+OK2/L8qrrXj5kxyoqmu+xYnHoDg2FMeG4thQnF4anvJCIigoCH/88QduvvlmGI1GpKWlIS0tDUajEXPnzsX27dsRFBTUlWN1afX19c4egu6xoTg2FNOd/e64ojf8vIz4ObkcH/xQ0G3fx9l4DIpjQ3FsKI4NxemhodD5ksDAQLzxxhsoKSlBfn4+8vPzUVJSghUrViAwMLCrxkhE1OOFBZjwyPWxMBokrP0uH7/tLXf2kIiIqIfrkjdeSZKE0NBQhIaGav6iECIivRoc6407r+gNAHh+fTYO59Y6eURERNSTdXghoaoqHnzwQdx777249957kZWV1erjbrvtNmzevLnLBtgTyLKMhIQEzV9Qo2VsKI4NxTiq36TTgjBlbAgaGhU89V6mS118zWNQHBuKY0NxbChOLw07PLoff/wRzz//PF599VWkpqbCaDS2+rjDhw9j8uTJeOaZZ7pskK5OkiT4+fnxbI4ANhTHhmIc2e/miyMxJM4L+aUWLPs422UuvuYxKI4NxbGhODYUp5eGHV5IfPnll4iIiMC2bduwadMm9OrVq9XHffPNN3jrrbfwxBNP4Lfffuuygboym82GXbt2af7KfC1jQ3FsKMaR/QwGCQ9eEws/LyN+21uBz7cVd/v3dAQeg+LYUBwbimNDcXpp2OGFxG+//Yb//Oc/GDly5EkfJ0kS5syZg6eeegovvvii6Ph6DEVRnD0E3WNDcWwoxpH9gnzd8OA1MZAk4O3NuTiY4xrXS/AYFMeG4thQHBuK00PDDi8kUlNTceWVV3b4C99yyy3YtWvXKQ2KiIjaNyLBB9dODIPVpuKBN1Ox/LMcHCnS/u0CiYjINXR4IWE0GuHp6dnhL+zn54faWtf4FzIiIq267rxwXHJGEKyKis07SjB32QE8834m6i3a/5csIiLStw4vJNzd3Tv1Pi2r1ar5K821QpZlJCYmspcANhTHhmKc1c8gS5g/JQqr70/EP8aFwGyS8XNyOZ77MBM2RV8XYfMYFMeG4thQHBuK00vDDo9u1KhR2LRpU4e/8BdffIERI0ac0qB6IpPJ5Owh6B4bimNDMc7sF+Jvws2X9MKKfw5AoI8R2/dX4vVNR3V3Ryceg+LYUBwbimNDcXpo2OGFxE033YS77roLaWlp7T42LS0Nd999N+bNmyc0uJ5CURQkJSXp4qIarWJDcWwoRiv9wgJM+PeN8fAwyfhiezE2/Fzo1PF0hlYa6hkbimNDcWwoTi8NO7yQuOyyy3DWWWdh+PDhuPfee7F161YUFRXBarXCZrOhqKgIW7duxb333osRI0bgnHPOwSWXXNKdYyciolb0ifTEohmxMMjAqq/y8N73+bp7mxMREWlf658q14Y1a9Zg7ty5ePHFF/HSSy+1+hhVVXHrrbdi+fLlXTJAIiLqvFH9fHHvVdFYuiEb//0uH8kZ1XhgWgwCfd2cPTQiInIRnbqCw93dHe+++y5++uknzJgxA7GxsXB3d4fZbEZcXBxmzZqFbdu2YcWKFW1+8jURETnGuSMC8cKtfRHq74bdh6txx/KD2JNe7exhERGRi5BUvV2J1w2ys7OxePFiLFq0CNHR0Q7//qqqQlEUyLKs+Y9C1yo2FMeGYrTcr6rOipc+zsFv+ypgMkr418w4jOrn6+xhtaDlhnrBhuLYUBwbitNLQ23fU6oHsVgszh6C7rGhODYUo9V+Ph5GLJoRi2njQ2Gxqnh8TQa2p1Q4e1it0mpDPWFDcWwojg3F6aEhFxIaoCgKUlJSNH9lvpaxoTg2FKP1fpIk4cZJEbj+/HBYbSqeei8DvySXO3tYzWi9oR6woTg2FMeG4vTSkAsJIqIeQpIkzDgvHHMuioBNAZ79IBM/7Cp19rCIiEinuJAgIuphrh4fhtsu6wVFBV74KBtf/1ni7CEREZEOcSGhEVr/CHQ9YENxbChGT/2uGBuC+Vf2BgC8+EkOPvutCIoGPmtCTw21ig3FsaE4NhSnh4a8axOcf9cmIiJn+W5nKZZtyIaiAjFhZlx9TijGDwuA0aDdu4QQEZE2aH+p0wOoqoqKigpwTXfq2FAcG4rRa7/zRwbiXzPjEBXijqyCerzwUTZuXrIfh47UOnwsem2oJWwojg3FsaE4vTTkQkIDFEVBWlqa5q/M1zI2FMeGYvTc74yBfnj97gF45PpY9OvtiYIyCxatPIy0XMcuJvTcUCvYUBwbimNDcXppyIUEERFBliWMHeSPZfP64oJRgaiut2HRysPIyK9z9tCIiEijuJAgIiI7WZbwz6lROHd4ACprbVj49mFkF9Y7e1hERKRBXEhohNlsdvYQdI8NxbGhGFfpZ5Al3HtVNMYN8UdFjRWLVh1GUYVjPmHVVRo6ExuKY0NxbChODw01d9emjIwMLF68GJWVlbBYLBgxYgQWLFgALy+vkz5v5syZrW5funQpQkJCTvpc3rWJiKglq03FE2vS8dehKsSGmfH8rQnw9jA6e1hERKQRmjojUVZWhpkzZ2L06NFYv349NmzYgKysLCxYsKBDz1+7dm2L/7W3iNACRVFQXFys+QtqtIwNxbGhGFfsZzRIWDTj2AXYmQX1eGJtBiyN3Tc/V2zoaGwojg3FsaE4vTTU1EJi7dq1qKurw5w5cwAARqMR8+bNww8//ICdO3c6eXTdR1VVZGVlaf4WX1rGhuLYUIyr9jObDHjihnhEBpmwN6MGj7xzGClZNd3yvVy1oSOxoTg2FMeG4vTSUFMLia1btyIxMREmk8m+bdiwYZBlGVu3bnXewIiIejB/byOemtMHQb5uSM6owX2vp+L+N1KxK63K2UMjIiIn0tSbXbOysjBhwoRm20wmEwICApCZmdnu85977jkkJyfDarUiMjISN954I4YOHdrh72+z2WCz2QAAkiRBlmUoitJsNdi0velx7W2XZRmSJLW6HTh26qrpz2w2W7PtxzMYDFBVtdXtJ46xre2OnFNHtnf1nAC02H96n5Mj91PT9zrx8XqekyP3U9OYFEWBwWBwiTkdvz0i0B0r7uqLr/4qxWe/FWNvZg0eXnkYT9/UB0PjPLtkTse/FjpiTq5y7B2/vcnx49T7nBy9n5qee/zPtd7n1DRGR+2n9l4P9Tin9rZ39Zxaez109JwMBgPao6mFRG1tbbOzEU1MJhNqak5+Kr1///4YPXo0HnjgAQDAe++9h2nTpmHZsmW4+OKLO/T9Dx06hNLSUgBAUFAQYmNjkZ2djZKSEvtjIiIiEBkZifT0dFRWVtq3x8TEIDg4GAcOHEB9/f9ulZiQkAA/Pz/s2bOn2cHSdOYlKSnJvi05ORnDhw+HxWJBSkqKfbssyxgxYgQqKyuRlpZm3242mzFo0CCUlpYiKyvLvt3X1xd9+/ZFfn4+8vLy7NudMScADpnTkSNH7A1dZU6O3k9DhgyBl5eXvaErzMkZ+6mgoAC9e/d2qTk17af83CzEe1firvOBrQckbN0vY9mGbNw1SYWkNHTZnJKTk3nsneKcwsLCYDAYmv0c631OzthPvr6+qKqqQnp6usvMyRn7qaysDKGhoS41J0fvp+TkZKfNadSoUWiPpu7aNGrUKEyYMAFLlixptn3MmDEYPXo0Xn755U59vWnTpqG6uhqbN28+6eOa7tr00EMP2e/apNUVqiuuujknzolz0t+cFEXFo+9mYvfhapw/MgB3T+2t+zmdbDvnxDlxTpxTT5uT7s5IxMTEoLCwsNk2i8WCsrIyxMbGdvrrxcXF4csvv+zw4w0GQ4toTTu6tcd21XZFUZCfn4/w8HD7qenWHi9JUqvb2xpjZ7d35Zw6ur2r5gQc+5fg8PDwZo/R85wcvZ+OPw5P/N56ndPJtnf1nI7v11Vj7Ox2R+4ngwG496po3PbiAXy3swxjB/vjzIF+pzz2pvGfeAzy2OvcdkVRWn0t7OwYO7vdlfaToijIzc1FeHi4y8ypiaP2kyNfD13p2GsiSVKrr4fOmtPJaOpi6/HjxyMlJQUWy/8++KjpFNH48ePbfN7BgwexYsWKFtvz8vIQFhbWLWPtSqqqIi8vT/NX5msZG4pjQzE9sV+ovwm3XdYLAPDyJzkoq2oU+no9sWFXY0NxbCiODcXppaGmFhKzZs2Ch4cHVq9eDQCwWq1YsWIFJk6c2Ox9WgsXLsTkyZPR0HDsPbnl5eVYtWpVs/cybt26FX/88Yf9VrJERNT1LhgViNMH+KKs2or7Xk/FkaL69p9EREQuQVNvbQoICMCaNWuwePFifP/992hoaMDw4cNx//33N3tcQ0MD6uvr7au0AQMGYNasWXjwwQdhNpvR2HjsX8VeeuklTJo0yeHzICLqKSRJwv3TovHkfzOxJ70a972eisdmxSMxxsvZQyMiom6mqYUEAMTHx2PlypUnfczSpUub/befnx/mz5+P+fPnd+fQuo0kSQgKCmp26z7qHDYUx4ZienI/bw8jnpwdj2UbcrB1dxkeejsNt1/eG5NGB3aqR09u2FXYUBwbimNDcXppqKm7NjlL012bFi1aZL9rExERdY6qqljzbT4+/LEAADAkzgt3XRmF3iFmJ4+MiIi6g6aukeipFEVBZmZmi1t9UcexoTg2FMN+x/4F7YYLI/DEDfEI9T/2KdjzXjqIT34pbP/JYMOuwIbi2FAcG4rTS0MuJDRAVVWUlJRo/sp8LWNDcWwohv3+5/QBvnjjngGYOi4EiqLirc25+GJ7cbvPY0NxbCiODcWxoTi9NORCgoiIupzZZMAtl/TCw9fFQpKAFZ8fwe/7yp09LCIi6kJcSBARUbcZO9gf8yb3gqICz32YhX2Z1c4eEhERdREuJDRAkiRERERo/sp8LWNDcWwohv3aNvmsEEyfEAqLVcXjazKQdrS21cexoTg2FMeG4thQnF4aciGhAbIsIzIyss2PMqf2saE4NhTDfid3w4URuHB0IKrrbHjo7TQcyK5p8Rg2FMeG4thQHBuK00tDbY+uh7DZbEhNTYXNZnP2UHSLDcWxoRj2OzlJkvDPK6Nw0WmBqKlX8PDKw9ib0fxtTmwojg3FsaE4NhSnl4ZcSGhEZWWls4ege2wojg3FsN/JybKE+VOiMPnMYNRZFDzyTjr+ONC8GRuKY0NxbCiODcXpoSEXEkRE5DCyLGHe5b0wdVwIGhoVPLEmHZ/+Wqj5WxwSEVFLXEgQEZFDSZKEWy7phXmTewEA3vwyFy9/moNGq7Y/eImIiJozOnsAdOwv1ZiYGM1fma9lbCiODcWwX+ddPiYEvYLd8fT7mfjqz1L8nVqF84YGom+9Al8vg7OHp0s8DsWxoTg2FKeXhpLK88nIzs7G4sWLsWjRIkRHRzt7OEREPUp2YT2WfJSNQ0eO3RbWbJJxxZgQzDgvDG5GnjgnItIqvkJrgM1mw759+zR/Zb6WsaE4NhTDfqcuOtSMF2/vi+dujseQKAmWRgXrthbg/jfSkF/a4Ozh6QqPQ3FsKI4NxemlIRcSGlFfX+/sIegeG4pjQzHsd+okScKgWC9MP8OKl+5IQFSoOw4eqcWdyw/i173lzh6ervA4FMeG4thQnB4aciFBRESaEhfugZfv6IcLRh37zInF72Vi6+4yZw+LiIhOwIUEERFpjtlkwL1XReOOK3oDAJZ8lI3kEz7AjoiInIsLCQ2QZRkJCQma/xh0LWNDcWwohv3EtdbwsjODMeO8MFhtKv69JgPZhdo/1e9MPA7FsaE4NhSnl4a8/asGSJIEPz8/Zw9D19hQHBuKYT9xbTWccV44Cssb8e3fpXh45WEMiPJEnUWBTVFx5dgQnDGQ3ZvwOBTHhuLYUJxeGmp7mdND2Gw27Nq1S/NX5msZG4pjQzHsJ66thpIk4a4rozAiwQcllY34bV8FdqZWYffhajz1Xib2pPMtT014HIpjQ3FsKE4vDXlGQiMUhZ/oKooNxbGhGPYT11ZDo0HCv2+Mty8aPEwydhyowLqthXhybQaWzOuL6FCzI4eqWTwOxbGhODYUp4eGXEgQEZEuGA0SRvb1sf/3gGhPlFZZ8e3fpfjX6nQsm9cXAT5uThwhEVHPwrc2ERGRLv3vLU/eKCizYNGqwyiqsDh7WEREPQYXEhogyzISExM1f2W+lrGhODYUw37iTqWh0SBh0Yw49O3lgYz8etz96iEcOlLbjaPUNh6H4thQHBuK00tDbY+uBzGZTM4egu6xoTg2FMN+4k6loZfZgOduScBZib4orbLigTdTsXV3GVRV7YYRah+PQ3FsKI4NxemhIRcSGqAoCpKSknRxUY1WsaE4NhTDfuJEGnq4G/DIjDhcdU4oGhpVPPdhFu54+SC++rME9Zaes094HIpjQ3FsKE4vDbmQICIilyDLEm66OBL3T4tGeIAJGfn1eOmTHMx6dh9WbslFQRmvnyAi6kq8axMREbmUc0cEYvywAPx5oBKfbyvCrrRqbPi5EJ/8UogzBvph1gXhiA33cPYwiYh0jwsJIiJyOQZZwpmJfjgz0Q/ZhfXYtK0Y3+0sxbaUCvx9qBLzLu+NSaMDIUmSs4dKRKRbktpTr0Y7TnZ2NhYvXoxFixYhOjra4d9fVVUoigJZlvmX2iliQ3FsKIb9xHV3w5p6G97/IR+f/FIEADh3eADunNIbHu6GLv9ezsLjUBwbimNDcXppyGskNMJi4Xt3RbGhODYUw37iurOhl9mAWy7phcdmxcHbw4Afkspw85L9+OCHfJRXN3bb93U0Hofi2FAcG4rTQ0MuJDRAURSkpKRo/sp8LWNDcWwohv3EOarhmQP98Mr8/hgW743SKivWfJuPmc+m4Jn3M7HxtyLsz6pBQ6M+9yOPQ3FsKI4NxemlIa+RICKiHicswIRnb0nAwZxabNpWhJ/2lOPn5GP/A4590N2k0YGYPiEMIf7av5c7EZEzcCFBREQ9Vv8oT/SPisFNF0ci6XA1Dh2ptf/vyx0l+PqvUkwaHYiLTg9CnwgPTb9XmYjI0biQ0AitfwS6HrChODYUw37inNUwwMcNE4cHYOLwAABAYbkF67YW4Ju/SvHljhJ8uaMEAT5GjO7niwtHBWJwnLdTxtkRPA7FsaE4NhSnh4a8axOcf9cmIiLSpsJyCzZtK8aOAxXIKWwAAEgScM2EMMw4PxwGmWcoiKjn0v5SpwdQVRUVFRXgmu7UsaE4NhTDfuK02DDU34SbLo7Em/cMxOoHEnHjpAgYZAkf/FiARSsPo7RKW3d70mJDvWFDcWwoTi8NuZDQAEVRkJaWpvkr87WMDcWxoRj2E6f1hmEBJkyfEIal8/oiPMCE3enVuG3ZAaz+Og/FFcdu06iqKrIK6rE1qcy+zZG03lAP2FAcG4rTS0NeI0FERNQJfXt5Yvn8fnj50yP4Jbkc67YWYMPPBRgc543sgnqUVVsBAN5mA+6+KgpjB/k7d8BERN2EZySIiIg6ydvDiIevi8Vrd/XHpNGBMMgSdh+uRlm1Fb1D3DG8jzeq62146r+ZeG3jEVh0+rkUREQnwzMSGmE2m509BN1jQ3FsKIb9xOmtYVyEB+7+RzRmXxSJgzm1iI8wI9jv2OdOfLezFK9uPIJN24uxM60K150bhvHDArr9Am29NdQiNhTHhuL00JB3bQLv2kRERN3jSFE9/rMuC6lH6wAAvUPccf354ThniD8/k4KIdI9vbdIARVFQXFys+QtqtIwNxbGhGPYT54oNe4eY8eLt/fDIjFjEhZtxpKgBz36QhSUfZaPe0vXzdMWGjsaG4thQnF4aciGhAaqqIisrS/O3+NIyNhTHhmLYT5yrNpRlCWMH++OV+f3x0LUx8PYw4PtdZbhnxSEcLW7o0u/lqg0diQ3FsaE4vTTkQoKIiMgBZFnC+KEBWH5nPyREeiAzvx53vXIQm/8ohk3R9i8LRESt4UKCiIjIgcID3bHktr646LQg1DYoWP7pEdzz2iEczKl19tCIiDpFcwuJjIwM3HzzzZg2bRqmTJmCJ554AjU1NZ36GitXrkT//v3xySefdNMou56vr6+zh6B7bCiODcWwn7ie0tDkJuOfU6Pw1Ox49Ap2R+rROtyz4hDe2nwUVpvY2Yme0rA7saE4NhSnh4aaWkiUlZVh5syZGD16NNavX48NGzYgKysLCxYs6PDXOHToEN55551uHGXXMxgM6Nu3LwwGg7OHoltsKI4NxbCfuJ7YcFQ/X7z2z/6YPSkCJqOET34pwsK301Ba2XhKX68nNuxqbCiODcXppaGmFhJr165FXV0d5syZAwAwGo2YN28efvjhB+zcubPd5zc2NuKhhx7CAw880N1D7VKKoiA3N1fzV+ZrGRuKY0Mx7CeupzY0GWVMmxCGZfP6ITLIhL2ZNbjzlYPYvKMY2/dXYH9WDY4U1aOq1gqlnWspemrDrsSG4thQnF4aauoD6bZu3YrExESYTCb7tmHDhkGWZWzduhUjR4486fNfeeUVnHXWWe0+ri02mw02mw0AIEkSZFmGoijNrphv2t70uPa2y7IMSZJa3Q4cO1BsNhvy8vIQHBwMNzc3+/bjGQwGqKra6vYTx9jWdkfOqSPbu3JOxzc0GAwuMSdH7ydVVZs1dIU5OXI/NR2DISEhLjOn9sbe1XOyWq3NjkFXmFNn9lNMmDuW3paAFz85gu37K7H8syM4kSwBAd5GjBsagItOC0DvYPdmY2/t55jHXufm1PSzHBoa2qGx62FOTWN01H5q7/VQj3Nqb3tXz6m110NHz6kjZ0M0tZDIysrChAkTmm0zmUwICAhAZmbmSZ+blJSErVu34qOPPkJhYeEpff9Dhw6htLQUABAUFITY2FhkZ2ejpKTE/piIiAhERkYiPT0dlZWV9u0xMTEIDg7GgQMHUF9fb9+ekJAAPz8/7Nmzp9nB0rRgSkpKsm9LTk7G8OHDYbFYkJKSYt8uyzJGjBiByspKpKWl2bebzWYMGjQIpaWlyMrKsm/39fVF3759kZ+fj7y8PPt2Z8wJgEPmdOTIEXtDV5mTo/fTkCFDmjV0hTk5Yz8VFBSgd+/eLjUnR++n5ORkl5sT0PH9dGkiEOklobjGBNnkjcLSGpRXWVBjAeoagJIqKz77rQif/VaE2GAVY/oqGBAJ9IqMQFhYmL2hluakt/0EAFVVVUhPT3eZOTljP5WVlSE0NNSl5uTo/ZScnOy0OY0aNQrt0dQnWw8cOBBTpkzBM88802z7hAkT0KdPH6xcubLV59XV1eHqq6/Gf/7zHyQmJuLIkSM477zz8Mwzz2Dq1Kntft+mT7Z+6KGH7J9s7egzEsnJyRgyZAjPSJzinBobG7Fnzx4MGTKEZyROcU6qqmL37t32hq4wJ0efkUhOTsbQoUPh5ubmEnNqb+xdPafGxkb7a2FPPCPR0TnllVrw9V9l+OavElTWHhtPdKg7rj4nFAm9zPjtzwMw+UTC18sNE4f5w2iUNT8nLe2npp/lYcOGtfj0cb3OqWmMjjwjcbLXQz3Oqb3tXT2n1l4PeUaiHZ6enrBYLC22WywWeHl5tfm8//znP7jkkkuQmJgo9P2bdtTxmnZ0a4/tqu2SJCEoKAhGo9H+otXa4yVJanV7W2Ps7PaunFNHt3fVnAwGg73h8Y/R85wcvZ8URWm1IaDfOZ1se1fPqennuOlrusKcumOMJ9tuNBpbHIN6n1N37KfeIR646WIPzDw/HD/tKcNHPxUiu7ABSzbkNH1XAAUAgO93leGha2IR6Ctrek7tbXfkfmr6WZZludXx6HFOTRy1nxz5euhKx14TSZJafT101pxORlMLiZiYmBZvS7JYLCgrK0NsbGybz/v5558RFhaGbdu2AQAaGo59Uuibb76JTz/9FFdeeWWHzkw4iyzLJ50ftY8NxbGhGPYTx4adY3KTccGoIJw3IhDb9lfgs9+KUFNnQ2SwOyKD3PFLcjmSM2pwx/KDeGB6DEYk+Dh7yLrA41AcG4rTS0NNLSTGjx+PNWvWwGKx2C+4bnqv2fjx49t83vfff9/sv5ve2jR37lxNLyCaKIqC7OxsREdHt7l6pJNjQ3FsKIb9xLHhqZFlCWMH+WPsIP/jGoZj2vhQLPs4B7/vq8CiVYcxfqg/rpkYjpgws7OHrGk8DsWxoTi9NNTUyGbNmgUPDw+sXr0awLE7eKxYsQITJ05sdsHHwoULMXnyZPuZB71TVRUlJSUt3vdGHceG4thQDPuJY0Nxxzf09jDikRmxuPWyXnB3k7F1dznmvXQAz7yfiSNF9e1/sR6Kx6E4NhSnl4aaWkgEBARgzZo12LFjB6ZPn46rrroKUVFRWLJkSbPHNTQ0oL6+vtW48+bNw7333gvg2FubZs6cidzcXIeMn4iISEskScKUsSF45/5ETJ8QCrObjJ+TyzHvpYN477t8WKzavkc9EWmbpt7aBADx8fFt3p2pydKlS9v8sxUrVnT1kIiIiHTN39uIGydF4h/jQvHhjwX47Pci/Pf7fGzdU4ZrJoShX29PRAa7wyBL7X8xIqL/p7mFRE8kSRIiIiJa3GaOOo4NxbGhGPYTx4bi2mvo42nELZf2wsQRAVj+6REcOlKLFz7KBgC4u0no19sTF4wKxLghATCbNPWmBYfhcSiODcXppaGmPkfCWZo+R2LRokX2z5EgIiJyZTZFxQ+7yrD7cBUO59Uhu7AeTbe093SXcd6IQMw4Pxx+Xvw3RyJqXc/85waNsdlsSE1NbfFBINRxbCiODcWwnzg2FNeZhgZZwgWjArFgWgxW/HMAPn18KBbNiMWovj6osyjYtL0Yd71yEAdzah0wcu3gcSiODcXppSEXEhpx/EeV06lhQ3FsKIb9xLGhuFNtaHKTcfZgfzw1pw9WLRiIkX19UFjeiAVvpGLLH9q/e0xX4nEojg3F6aEhFxJERETUTHigO/59YzyunRgGq03Fy5/m4Mn/ZiK3xDVuu05EXYMLCSIiImrBIEuYdWEEHp8VB19PA7alVODWZQewcksuauq1/XYLInIMLiQ0QJIkxMTEaP7KfC1jQ3FsKIb9xLGhuO5oeMZAP7y9YCCuGBMMVVWx4edCzHh6L579IBPb91eg0cU+i4LHoTg2FKeXhrxrE3jXJiIioo7IKazH6m/ysGN/BWz/v34I8DHi7qlROH2An3MHR0QOxzMSGmCz2bBv3z7NX5mvZWwojg3FsJ84NhTX3Q2jQs149Po4vPfwYNxxRW8MiPJEWZUVj72bgeWf5qDeov99x+NQHBuK00tD3hxaI+rr6509BN1jQ3FsKIb9xLGhOEc09PMy4rIzg3HpGUH45u9SvLHpKDb/UYK/DlWiX29P+Hoa4etlRGy4GQOivBDq76b5t2gcj8ehODYUp4eGXEgQERHRKZEkCZNGB2FovDdeWJ+NlKwaFJZXtHhcgI8RZyX6YcZ54Qj0cXPCSImoO3AhQUREREIiAt3x/NwEpOfVoazaiqpaK0qrrDicW4sD2bXIL7Ng844S/LirDNMnhuHKsSEwufHd1UR6x4WEBsiyjISEBMgyX1RPFRuKY0Mx7CeODcU5s6EsS0jo5dnqnxWUNWDtt/n4flcZVn+dh03binH+yECcPzIAvUPMLR5vaVTw3a5SWBpVDI33RmyYGbLsmLdG8TgUx4bi9NKQd20C79pERETkCAdzavHW5qPYl1lj39avtyfOHuyHsxL90CvYHT/tLsc7X+eisLzR/hgfDwPOGuSHeZN7wWwyOGPoRNQKbS9zegibzYZdu3Zp/sp8LWNDcWwohv3EsaE4rTfsH+WJF27ti1fv6o8rx4bA39uIQ0dqseqrPNyy9ACuWbwXz63LQmF5IxJjvDD5rGDEhplRVWfDN3+V4t9rM2Fp7N7PrdB6Qz1gQ3F6aci3NmmEorjWB/o4AxuKY0Mx7CeODcXpoWF8hAfmXtYLN10cid3p1dieUoHfUypQUtmIiEAT5lwcibGD/Ox3esorbcDCtw9jV1oVnvkgE4tmxMFo6L63OumhodaxoTg9NORCgoiIiJzCYJAwsq8PRvb1wbzLe6Gw3IIgX1OLRUJEoDuevqkPHngzDdv3V+L59Vl4YHoMDA66boKIWse3NhEREZHTSZKEsAD3Ns80RAYdW0z4eRnx855yPPdhFixW7f+LLZEr40JCA2RZRmJiouavzNcyNhTHhmLYTxwbinP1htGhZjx9Ux/4exvxS3I5Hn83HXUNXfsecldv6AhsKE4vDbU9uh7EZDI5ewi6x4bi2FAM+4ljQ3Gu3jA+wgMv3NoXof5u2JVWjYVvH0ZFjbVLv4erN3QENhSnh4ZcSGiAoihISkrSxUU1WsWG4thQDPuJY0NxPaVhr2B3LLmtL2LCzDh4pBZ3v3YIGfl1XfK1e0rD7sSG4vTSkAsJIiIi0p1gPxOen5uAofHeyC+14N4VqfgludzZwyLqUbiQICIiIl3y8TRi8Zw+uGJMMOotCp5+PxNLN2Rjx/4K1Fu0ff99IlfA278SERGRbhkNEm6b3Bt9Ij2w/LMj+PbvUnz7dyncjBJO6++LWy/rhVB/7b/XnEiPJFVVVWcPwtmys7OxePFiLFq0CNHR0Q7//qqqQlEUyLJs//Ad6hw2FMeGYthPHBuK6+kNC8oa8Pu+Cvx5sBLJGTWw2lR4mWXcfnlvTBwe0KEmPb1hV2BDcXppyLc2aYTFYnH2EHSPDcWxoRj2E8eG4npyw7AAd1x5diievikBHywahAtGBaKmXsHz67Px7AdZqKrt2N2denLDrsKG4vTQkAsJDVAUBSkpKZq/Ml/L2FAcG4phP3FsKI4N/8fbw4h7r4rGohmx8PU04Ofkcsx76SB2pVWd9HlsKI4NxemlIa+RICIiIpd19mB/JEZ7YdnH2fjrUBUeXnkYV4wJxnkjA+HuJsPdTUaQr1ubn6hNRG3jQoKIiIhcWqCvG/59Yzy+2F6MlVtysfH3Ymz8vfh/f+5jxD/GheKSM4LgZnDiQIl0hgsJjdD6R6DrARuKY0Mx7CeODcWxYeskScLks0IwvI8P3v8hH6VVVtRbFFTXWZFbYsFbm3Ox7qcCXHZGECLNbCiKx6E4PTTkXZvg/Ls2ERERkfPszazGuh8L8Neh/10/0T/KExOHB+Di04JgctP+L3REzsCfDA1QVRUVFRXgmu7UsaE4NhTDfuLYUBwbnprBsd54cnYfvHxnP1x6RhB8PGQczKnF65uO4p4VqThSVO/sIeoKj0NxemnIhYQGKIqCtLQ0zV+Zr2VsKI4NxbCfODYUx4Zi+vbyxLzJkVhwcSOemBWL+Agz0vPqMP+VQ/hhV6mzh6cbPA7F6aUhr5EgIiIiOo5BBob388HwBF+8tTkXX2wvxvPrs/Hf7/LRJ9IDfSI90WhVkJ5Xh/S8OribZNxxeW8M6+Pj7KETORQXEkREREStMLnJuOOK3hjWxxuvbTyCvFIL8kot+HVvRYvHLlx5GFefE4qZF0TwVrLUY3AhoRFms9nZQ9A9NhTHhmLYTxwbimNDcSc2PHuwP8YO8kNhuQVpuXXIyKuD0SAjPsID8RFm/La3Am9vycX6nwqRdLgaD18Xg7AA9za/vtV27H3vrrzg4HEoTg8Nedcm8K5NREREJCY9rw7PfZiF7MJ6+HoZ8OiMOAyO8272mIz8Onz9Zwl+2FWGhkYF0yaE4epzQnlXKNItHrkaoCgKiouLNX9BjZaxoTg2FMN+4thQHBuKO9WG8REeeOmOfjhnqD8qa2xYuPIwNv9RjL0Z1Vj9dR7uXH4Qt790EBt/L0ZVnQ2NNhX//S4ft714AH8erNT83Xk6g8ehOL005FubNEBVVWRlZSEgIMDZQ9EtNhTHhmLYTxwbimNDcSINzSYZD10Tg9gwM9Z8m4/lnx5p9ucBPkZcMDIQF44OQl2DDa9sPIKDObX41+p0RIeacd7IAJw7PADBfqaumo5T8DgUp5eGXEgQERERdRFJknDtueGIDjNj1ZZc+HgYMaqfD0b180X/3p4wHHddxNLb+uKbv0ux7scCZBfW452v8vDu13m46PQg3HJJJMwmgxNnQtQ+LiSIiIiIutjYQf4YO8j/pI+RZQkXnRaEC0cFYl9WDb7bWYqfdpdj844S7EmvxoPXxCAh0tMxAyY6BbxGQiN8fX2dPQTdY0NxbCiG/cSxoTg2FOfohrIsYUicN+75RzReu6s/+vf2xJGiBtzzWipWfZWLrII6h46nK/A4FKeHhrxrE3jXJiIiItIOq03F+9/nY93WAij//1tadKgZE4cH4IoxwfBw51ueSBt4RkIDFEVBbm6u5q/M1zI2FMeGYthPHBuKY0NxWmhoNEiYdWEEXpnfH1PHhSDEzw3ZhfV495s83LxkP779uxSKot1/B9ZCQ73TS0MuJDRAVVXk5eW51K3fHI0NxbGhGPYTx4bi2FCclhrGRXjglkt64d0HE7Hktr4YkeCN0iorlm7Ixt2vHcLPe8rQaNXeL5paaqhXemmouYutMzIysHjxYlRWVsJisWDEiBFYsGABvLy82nyOzWbD22+/jV9//RVubm6oq6tDdXU1pk6ditmzZztw9ERERERdS5IkJMZ4YfGcPvjjQCXe+jIXqUfr8MwHWQjwNuLC0UG4YkwwAnzcnD1U6mE0tZAoKyvDzJkzcf311+O2226D1WrF3LlzsWDBAqxYsaLN59XX12P58uX46KOPMHDgQADAnj17MG3aNHh6emL69OmOmgIRERFRt5AkCWcM9MPIvj74eU85Nv9RgpSsGqzbWoAvdxTjtst64dwRAZAkqc2voaoqdqZW4du/S3Hh6CCM7OvjwBmQq9HUQmLt2rWoq6vDnDlzAABGoxHz5s3D9ddfj507d2LkyJGtPs9sNuPdd9+1LyIAYOjQofD19cXhw4cdMnYRkiQhKCjopD/4dHJsKI4NxbCfODYUx4bi9NDQzSjjvJGBOG9kIDLy6vDBjwX4JbkcL3yUjZ/2lOPOKb0R6t/8Q+1UVcWfByvx/vcFOHikFgCwfX8lXrgtoctvMauHhlqnl4aaumvT1KlT4eXlhbVr19q3WSwWDBs2DLfccgvuvffeDn2dxsZGfPjhh3jttdewdu1aJCQknPTxvGsTERER6dnv+8rxysYjKKuywiAD44YEYMrYYIQHuuO7naXY8kcJjhY3AABC/d0QH+GB7fsrEeznhpfu6IdAvi2KToGmzkhkZWVhwoQJzbaZTCYEBAQgMzOzQ1/jjjvuwPbt2xEVFYXVq1e3u4g4ns1mg81mA3BsJSjLMhRFaXahS9P2pse1t12WZUiS1Op24NhV+YqiICcnB1FRUTAajfbtxzMYDFBVtdXtJ46xre2OnFNHtnflnKxWK7KzsxEVFWUfn97n5Oj9BBxbVPfu3dv+GL3PyZH7qennODo6Gkaj0SXm1N7Yu3pOVqvV/looy7JLzMnR+wk49nfp8T/Hep+To/eToig4evQooqKicCItz+mMAT4YFNMPa74twLc7S7F1dxm27i6DLANN0+sd7I6p44Jx7vAASBLw77WZ+Du1Gv9em4Fn5sTB5Nb8tf9U59Te6yGPvfbn1NrroaPnZDC0f5thTS0kamtrYTKZWmw3mUyoqanp0Nd49dVXYbVasXLlSlx33XV48803MWrUqA4999ChQygtLQUABAUFITY2FtnZ2SgpKbE/JiIiApGRkUhPT0dlZaV9e0xMDIKDg3HgwAHU19fbtyckJMDPzw979uxpdrAkJibCZDIhKSnJvq2srAzDhw+HxWJBSkqKfbssyxgxYgQqKyuRlpZm3242mzFo0CCUlpYiKyvLvt3X1xd9+/ZFfn4+8vLy7NudMScADplTTk4OysrKUFZW5jJzcvR+GjJkCEpLS+0/A64wJ2fsJ5PJhN69e7vUnBy9n8rKylxuToBj9lNYWFiLn2O9z8kZ+0lRFPj5+SE9PV13c7r98uG4epw/3vsqFTsOS6i1AEOjgRmT+iI60IrDhw9jb3I2AGDKCHcUVrjjYE4tnngnGVedpkKSum5O3t7eCA0N5bEnMKeysjKnzakjvz9r6q1No0aNwoQJE7BkyZJm28eMGYPRo0fj5Zdf7tTXmzp1Ktzc3LBu3bqTPq7prU0PPfSQ/a1Njlyh2mw2JCcnY8iQIXBzc7NvP54rr7q7Yk6NjY3Ys2cPhgwZAoPB4BJzcvR+UlUVu3fvtjd0hTk5cj81/RwPHToUbm5uLjGn9sbe1XNqbGy0vxYaDAaXmJOj95OqqkhKSmr2c6z3OTl6PzX9LA8bNqzF+9P1NiebokJRVLgZ2z7DV1Buxd2vHkJVnQ23XhqByWcFC8+pvddDHnvtz6m110OekWhHTEwMCgsLm22zWCwoKytDbGxsm8+z2WxQVdX+tqAmCQkJ+Prrrzv8/Zt21PGadnRrj+3q7U2/ALf1eEmSWt3e1hg7u7075tTe9q6e04n70BXm1B1jbG170wtIaz8Hep3TybZ315yOPxZFx9jZ7a6yn44/Bl1lTsfrzjmd7OdYr3PqqjF2drsrzOnEP2ptTpFBBjx4TQweXZ2Ot7fkIaG3FwbHep907B2dkyNeD11hP53o+Dkd/7PsrDmdjKY+kG78+PFISUmBxWKxb2s6RTR+/Pg2n7dx40Y89dRTLbYXFBTA39+/O4bapSRJQkREhOavzNcyNhTHhmLYTxwbimNDcT2x4ah+vph5QThsCvD0+5korWyEzaYiM78Ov+8rx87UKhzMqUVucQNsHfhE7Z7YsKvppaGmzkjMmjULH330EVavXo25c+fCarVixYoVmDhxYrP3aS1cuBB79+7Fhg0b4O7uDgDYsmULZs2ahfj4eADA999/jx07duCee+5xylw6Q5ZlREZGOnsYusaG4thQDPuJY0NxbCiupzacPj4Mh3JqsX1/Je5cfhC1DTY0NLZcNHibDRgS740RCd4YO8gfgb4t7/bUUxt2Jb001NQZiYCAAKxZswY7duzA9OnTcdVVVyEqKqrFNRMNDQ2or6+3v+9rzJgxmDp1Ku69917MmDED11xzDV5//XU888wzuPXWW50xlU6x2WxITU1t8d406jg2FMeGYthPHBuKY0NxPbWhLEtYMC0GvYLdUVZtRaNNRVy4GeOH+uOsRD8Mi/dGdKgZNQ02bEupwGufH8W8lw8gPa+uxdfqqQ27kl4aauqMBADEx8dj5cqVJ33M0qVLm/13eHg4Hnzwwe4cVrc7/up5OjVsKI4NxbCfODYUx4biempDL7MBy27vi/xSC6JDzXB3a/nvzRU1VuxJr8a3f5fiz4OVeOitNDx9Ux8k9Gr+oXY9tWFX0kNDTZ2RICIiIiLn8fEwom8vz1YXEQDg52XEuCH+eGxWHM4bEYCqOhsWvn0YB3NqHTxS0gIuJIiIiIioUwyyhHuuisaFowNRXW/DwrfTsGlbEZQOXIxNroMLCQ2QJAkxMTGavzJfy9hQHBuKYT9xbCiODcWxYccZZAn/vDIKV4wJRp1FwWufH8WCN1KRXdjAhoL0chxyIaEBsiwjODi4zfv9UvvYUBwbimE/cWwojg3FsWHnyLKE2yb3xrM390FkkAn7s2tx16up+HRHA2oalPa/ALVKL8ehtkfXQ9hsNuzbt0/zV+ZrGRuKY0Mx7CeODcWxoTg2PDXD+vjgtX8OwPQJoVBVFZ/8WoSbnt+Pjb8XwdLIBUVn6eU41Nxdm3qq+vp6Zw9B99hQHBuKYT9xbCiODcWx4alxd5Nx46RInDvcHy+tP4iUXBte33QU73yVh+EJ3jitvy+GxHkjMsgdRoO237KjBXo4DrmQICIiIqIu0yvYHdeNUSD7JmDdT0VIzqjGjv2V2LH/2O1MjQYJvYLdMTDaE9dMDEdYgMnJI6ZTxYUEEREREXW5ofHeGNHXD9V1VuxKq8afByuRdrQWOUUNyCqoR1ZBPb7fVYZ/jAvFtPGh8HA3OHvI1ElcSGiALMtISEjQ/AU1WsaG4thQDPuJY0NxbCiODcWd2NDb49hnT4wb4g8AsNlU5JY04PNtxdi8oxgf/liAb/4qwTUTwzBpdBBMbXyGRU+il+NQUlW1x9/wNzs7G4sXL8aiRYsQHR3t7OEQERER9QiZ+XV488tc7EqrAgAE+brh6vGhmDQ6EGYTz1BonbaXOT2EzWbDrl27NH9lvpaxoTg2FMN+4thQHBuKY0NxnWkYG+6BxXPi8ezNfTA4zgsllY14fdNRXPPUPjz3YSa2p1SgoQfe9UkvxyHf2qQRitLzfki6GhuKY0Mx7CeODcWxoTg2FNeZhpIkYVgfHwzr44M96VX46KdC7Eyrwtbd5di6uxySBIT6m9A72B3xkR4Yk+iH/lGemv+wNlF6OA65kCAiIiIiTRga74Oh8T4or27Er8kV+Dm5DGm5dSgos6CgzIK/U48tNEL83DBuiD+umRgGH0/+OussLE9EREREmuLv7YbLzgrGZWcFQ1VVlFVZkVNUjz3p1fh1bwWyC+vxya9F2JZSgcdviEd0qNnZQ+6ReLE1nH+xtaqqqK+vh9lsdvnTdN2FDcWxoRj2E8eG4thQHBuKc0TDrIJ6vLIxB3szauDpLmPhtbEY3d+3W76XM+jlOOQZCY0wmfhhLKLYUBwbimE/cWwojg3FsaG47m4YE2bG03P64LXPj+CrP0vx2LvpOG9kIM4e7I/hCd6wNCr4fV8Fft5TjpKqRvTr7YnEGC8MjvVGr2D3bh1bV9HDcci7NmmAoihISkrSxUU1WsWG4thQDPuJY0NxbCiODcU5qqGbUcZdV0bh1st6ARLw7d/HFhTXPrUX1y7eh2Uf5+Dv1Cpk5tfjm79K8eLHObh5yX48+s5hHDpS261jE6WX45BnJIiIiIhIlyRJwpSxIRg72A+/7a3Ab3vLsS+rBkaDhLGD/DB+WACiQ91xIKcWKVk12L6/An8dqsJfh6pwVqIvZl0QgdhwD2dPQ7e4kCAiIiIiXQvxM2HK2BBMGRuCqlorjAYJHu7/+0C7mDAPTBodhNoGGz7/vRgf/1KIbSmV2LG/EheODsLMC8IR6OPmxBnoExcSREREROQyTnY7WE93A66ZGIbJZwVjw8+F+PTXQnz1Zwl+2l2GC0cHYmi8NwbHesPXi78idwTv2gRt3LVJURTIsqzpK/O1jA3FsaEY9hPHhuLYUBwbitNTw6IKC979Og8/JJXh+N+I4yPMGDvIH+OG+CPKCbeW1UtDLrc0wmKxwGzmPZBFsKE4NhTDfuLYUBwbimNDcXppGOJnwoJpMbh6fBj+OlSJvRnV2JtZg/S8eqTn5WPtd/mIDTfj0tODcd7IgGZvl+puemjIuzZpgKIoSElJ0fyV+VrGhuLYUAz7iWNDcWwojg3F6bFhTJgZ/xgXisdmxWPdI4Pxwq0JuGJMMIJ83ZCZX49XPz+CWc+m4O3NR3GkqL7Zc4sqLFj9dS7+vTYDb20+is1/FONgTo3QePTSkGckiIiIiIj+nyxLGBTrjUGx3ph7aS/sSqvCZ78V4a9DVfj4lyJ8/EsR4sLNGDfUH0cKG/DTnjLYWvl9f9YF4bj23HDHT8CBuJAgIiIiImqFLEsY1c8Xo/r54khRPb7cUYJfksuRkV+PjPx8AICbUcIFowJxxkBfFJVbkFPUgC1/lGDNt/mIDjt2rYWr4kJCI2SZ7zITxYbi2FAM+4ljQ3FsKI4Nxbliw94hZtx6WS/cckkkUrJqsC2lAt6eBlx8WhD8vZvfOjY+wgMvfZKDF9ZnI3KeO+JO4bMq9NCQd22C8+/aRERERESuZcXnR/D5tmKEBZgw64JwVNXaUFFrRXyEB8YO8tP03Zg6imckNEBVVVRWVsLX19clDipnYENxbCiG/cSxoTg2FMeG4tjwmFsu7YWswnrsPlyN59dnN/uzAVGeuG1yL/SP8mr1uXppyIWEBiiKgrS0NAwfPhwGg+NuK+ZK2FAcG4phP3FsKI4NxbGhODY8xmiQsOi6WKz+Og+KCvh5GWA2GfD1nyU4kFOLu19Lxai+PvDzMsLkJsHNKMNklGAyynA3SQgzHsW4s7TdkAsJIiIiIqJu4ONpxPwro5ptm3p2CD77vQgf/lCAv1Or2nzuuP4Sxp3V3SMUw4UEEREREZGDmNxkTBsfhkmjg5B2tBYWqwqLVYGlUUWjVYHFqsJmUxAoH3H2UNvFhYRGaP2TC/WADcWxoRj2E8eG4thQHBuKY8P2+XkZMaqfb6t/ZrPZcOBAsYNH1Hm8axN41yYiIiIios7S/g1qewBFUVBcXKz5j0HXMjYUx4Zi2E8cG4pjQ3FsKI4NxemlIRcSGqCqKrKyssCTQ6eODcWxoRj2E8eG4thQHBuKY0NxemnIhQQREREREXUaFxJERERERNRpXEhohK9v61ftU8exoTg2FMN+4thQHBuKY0NxbChODw151ybwrk1ERERERJ3FMxIaoCgKcnNzNX9lvpaxoTg2FMN+4thQHBuKY0NxbChOLw25kNAAVVWRl5en+SvztYwNxbGhGPYTx4bi2FAcG4pjQ3F6aciFBBERERERdRoXEkRERERE1GlcSGiAJEkICgqCJEnOHopusaE4NhTDfuLYUBwbimNDcWwoTi8Njc4eAAGyLCM2NtbZw9A1NhTHhmLYTxwbimNDcWwojg3F6aUhz0hogKIoyMzM1PyV+VrGhuLYUAz7iWNDcWwojg3FsaE4vTTkQkIDVFVFSUmJ5q/M1zI2FMeGYthPHBuKY0NxbCiODcXppSEXEkRERERE1Gm8RgJAY2MjACAvL88p399ms6G4uBjZ2dkwGAxOGYPesaE4NhTDfuLYUBwbimNDcWwoTisNw8PDYTKZ2vxzLiQAFBcXAwBWrVrl5JEQEREREWnDokWLEB0d3eafS6rW33zlANXV1di3bx+Cg4Ph5ubm7OEQERERETlde2ckuJAgIiIiIqJO48XWRERERETUaVxIEBERERFRp3EhQUREREREncaFBBERERERdRoXEkRERERE1Gn8HAknysjIwOLFi1FZWQmLxYIRI0ZgwYIF8PLycvbQNGfHjh348MMPUVRUBFVVUV1djQsvvBA33XQTzGYzAGD58uX47rvv4Ovr2+y5t9xyC8455xxnDFtTduzYgYULF6JXr17Nto8bNw5z5861//enn36KtWvXwsPDA3V1dZg9ezYmT57s6OFq0syZM9HQ0AB3d/dm2/fu3Ys5c+Zg/vz5eOihh5Cent7iMQ8//DAGDhzoyOFqynfffYcnn3wSZ511Fp599tkWf/7TTz9h+fLlcHd3R01NDaZMmYIbb7yxxePefvttfPHFF/Dy8oLFYsHdd9+NsWPHOmAGztVWP6vVii+++AIbN26EoihoaGiAzWbDzJkzcfnllzf7GjNnzmz1ay9duhQhISHdOn4tONkx2NG/PywWC5YvX45ffvkFHh4eMBgMeOihhzB48GCHzMHZTtZwyJAhGD58eLNtNpsNf//9N9asWYMzzjgDAHDRRRe1erytXbu228btbB35HQbQ6eugSk5RWlqqjh07Vl2xYoWqqqra2Niozp49W73tttucPDJtOv/889UlS5aoiqKoqqqqGRkZ6mmnnabedddd9se8/PLL6vbt2501RM3bvn27+vLLL5/0MZ9//rk6fPhwNT09XVVVVU1LS1OHDx+ufv31144YouZdf/31ak5OTrNtJSUl6pAhQ9TMzExVVVX1wQcfbPGYnqy2tla9/fbb1fvuu08966yz1AcffLDFY/7880910KBB6p9//qmqqqoWFhaqY8eOVd95551mj3v99dfVc845Ry0uLlZVVVW3bdumDh48WE1KSur2eThLe/3y8vLUAQMGqFu3brVv27Jli9qvXz/1v//9b7PHXn/99Q4Zs9Z05Bjs6N8fjz76qHrFFVeoNTU1qqqq6ieffKKOGjVKzc7O7vJxa0lHGk6cOLHFtu+++06dOHGiarPZ7Nt64nHYkd9h9Po6yLc2OcnatWtRV1eHOXPmAACMRiPmzZuHH374ATt37nTy6LSnX79+uPnmmyFJEgAgNjYWF198Mb755hvU1NQ4eXSuQVVVLFu2DJMnT0ZcXBwAoE+fPrjooouwZMkSJ49OG55++mmEhYU12/bJJ5/gtNNOQ0xMjJNGpW319fWYMWMGXnjhhWb/8na8F198EWeccQZGjx4NAAgJCcE111yD5cuXo76+HgBQU1OD119/Hddddx2CgoIAAGeeeSZGjBiBl156yTGTcYL2+rm5ueGiiy7C+PHj7dsuuugixMfH49NPP3XkUDWrI8dgR2RnZ2P9+vW4+eab4enpCQC48sor4e/vjzfffLOrhqtJHWm4cuXKFtvWrVuH6dOnQ5Z79q+bHfkdRq+vgz17zzrR1q1bkZiY2OzTAocNGwZZlrF161bnDUyjXn311RannM1mMyRJgsFgcNKoXEtqaiqOHj2KESNGNNs+cuRIZGZmIiMjw0kj046oqCi4ubnZ/1tVVaxfvx7XXnutE0elbQEBARgzZkybf15dXY2//vqr1eOu6c8A4I8//kBtbW2Lx40YMQLbt29HXV1d1w9eA9rrFxQUhGXLlrXYbjabYTTy3ctA+w076ueff4aqqi2OweHDh+PHH38U/vpa1pGGTf8A1eTo0aPYsWMHrrrqqu4cmi609zuMnl8H+SrjJFlZWZgwYUKzbSaTCQEBAcjMzHTKmPTmzz//xKRJk5r968hnn32GV155BVarFT4+PpgyZQouueQSJ45SW5KSknDLLbegtrYWRqMRY8aMwQ033ACz2YysrCwAQGhoaLPnNP13ZmZmi78oerpt27bBYrFg4sSJzbavXLkSqampsFqtCAoKwnXXXdcj3sd/KrKzs6GqaovjrunMT2ZmJs4+++w2j8+wsDDYbDbk5OSgX79+jhm0xpWVlSE1NRWLFi1q8WfPPfcckpOTYbVaERkZiRtvvBFDhw51wii1p72/P5r+bm7tGCwqKkJNTQ2vcTzO+vXrcf7559v/5bxJbW0t/vWvfyE9PR1WqxVxcXGYO3duj/v75fjfYVJSUnT7OsiFhJPU1tY2OxvRxGQy8a06HbB582YUFBTgjTfesG+LiIiAu7s7nnjiCZhMJvz111+49dZb8ffff+PRRx914mi1wcfHB2FhYbj//vsREBCA3NxczJs3D19//TXWrVtnP+5OPC6b/ru2ttbhY9a6Dz/8ENOmTWt2Viw2NhZBQUF45JFHYDAY8O233+KWW27BggUL7G9lpP9pOq7aO+54fHbciy++iJEjR2L69OnNtvfv3x+jR4/GAw88AAB47733MG3aNCxbtgwXX3yxM4aqGR35+6O2thaSJDU7Kwk0Pwa5kDjGarXi448/xosvvtjiz+Li4nD55Zdj9OjRaGxstL+lds2aNRg5cqTjB+sEJ/4Oo+fXQb61yUk8PT1hsVhabLdYLHwhaseePXvwn//8B2+//XazOz9cddVVmDt3rv0HavTo0bj22mvx3nvvoaioyFnD1YzExEQ8/fTTCAgIAABERkbivvvuw759+/Dtt9/aj7sTj8um/256TzAdU1RUhJ9//hlXX311s+233XYbrr76avvi4oILLsCkSZPw6quvwmq1OmOomtZ0XLV33PH47JgPPvgAe/bswSuvvNLifemPPPIIzjvvPEiSBEmScP3112Po0KFYvny5k0arHR35+8PT0xOqqqKxsbHZc3kMtvT9998jICDA/n7/473wwgv27W5ubrjvvvsQGBiIFStWOHqYTtHa7zB6fh3kQsJJYmJiUFhY2GybxWJBWVkZYmNjnTMoHdizZw/uv/9+rFixokO30oyLi4Oqqjhy5IgDRqc/TaeSc3Jy7BcLn3hcNv03j8vmPv74Y0yYMKFDt82Mi4tDdXU1SktLHTAyfYmOjoYkSe0edyc7Pg0GA6Kiorp/sBr34Ycf4tNPP8Xq1atbvB+7LXFxccjOzu7mkenTiX9/NB2LrR2DISEh/EfA46xbt67D144ZDAZER0f3iOOwrd9h9Pw6yIWEk4wfPx4pKSnNVpV79uyBoijN7r5B//P333/jgQcewKuvvmr/AdyyZQtycnIAAPfcc0+L5+Tl5QFAizvt9ERLliyxt2qSn58P4Fifvn37olevXti1a1ezx+zatQuxsbE97v2rJ6MoCtavX4/rrruu2faSkhI89dRTLR6fl5cHd3d3+Pv7O2iE+uHt7Y1Ro0a1OO527twJb29v+79cnn766fDw8EBSUlKzx+3atQtnnHEGPDw8HDVkTVqzZg02bdqEVatWwc/PDwCa3Uno4MGDrf6Lb15eHl8f0bG/P8455xxIktTiGExKSmpxzWNPlp2djd27d+OKK65o8Wfbtm3D+vXrW2zPz893+ePwZL/D6Pl1kAsJJ5k1axY8PDywevVqAMfeT7hixQpMnDgRo0aNcu7gNGj79u248847MX/+fNTV1SE5ORnJycnYuHEjcnNzARx7z+HmzZvtz8nOzsaHH36ISZMmITIy0llD14ykpCSsXr0aNpsNwLG75bz66qvo1asXLrjgAkiShHvuuQdffPGF/aLCw4cPY8uWLbj33nudOHLtafowqtNPP73Z9rq6Onz44Yf2O2wAxz6sbvPmzZgxY0ar10URcPfdd+OPP/7A33//DQAoLi7Ghx9+iDvvvNN+MwUvLy/cdttteP/99+1ndv744w/s3LkTd999t7OGrglvvfUW1qxZg7vvvhsZGRn218c1a9bYH1NeXo5Vq1YhPT3dvm3r1q34448/eO0OOvb3R3R0NK6++mqsXLnSfnecjRs3orS0FLfeeqtTxq1F69atw+TJk1s9Q5OXl4c33nij2duN33vvPRw5cqTVD15zFR35HUavr4OSqqqqU74zIT09HYsXL0Z1dTUaGhowfPhw3H///Tw92oqzzjqrzbeFNH1i5vvvv48tW7bYf1Gur6/HRRddhBtvvJG/wAH49ddfsX79evu/jtfW1mLIkCG48847m70955NPPsHatWvh6emJ2tpazJ49u8Un5PZ0t99+O8aMGYPrr7++2faGhgasXbsW3333HYxGI2w2GxobG3H11Vfj6quv7rH3Ul+0aBGys7ORlJQEX19fxMfHY9KkSc36deQTXVVVxcqVK7Fp0yZ4e3vDYrHgn//8J84++2wHz8ixTtYvLS0Nl156aZvPPXjwIACgoqICa9aswc8//wyz2Wx/n//s2bMxadIkh8zDmdo7Bjv694fFYsHLL7+MX375BZ6enjAYDHjwwQcxZMgQp8zLkTryc2yxWDBhwgSsXr261bsH5efn491338Uff/xhv1bUbDbjtttuw1lnneXI6ThUR36HAfT5OsiFBBERERERdVrP/OcxIiIiIiISwoUEERERERF1GhcSRERERETUaVxIEBERERFRp3EhQUREREREncaFBBERERERdRoXEkRERERE1GlcSBARERERUadxIUFE5ML69OmDwMBASJIEDw8PhIeH44svvnD2sE7Z22+/jaCgIOzZs8fZQyEi6vG4kCAicmGHDx/GJ598AgCYPn068vPzcdlllwEAbrzxRkiS5MzhtWrChAmIjY1t9c/Ky8tRUVGB2tpaxw6KiIha4EKCiIh0Y8GCBaioqMCZZ57p7KEQEfV4XEgQEZGueHl5OXsIREQELiSIiHqc0tJShIeHY926dQCA8PBw+//S0tLsj0tKSsLll1+OoKAgBAYGYsiQIVi6dCkURbE/ZubMmQgJCYEkSXj88cexatUqDB06FD4+PpAkCatXrwYAvPrqqzj33HMRFRWFwMBA9O/fH8888wxsNpv9a+3evRvh4eH4/fffkZOT02xcDQ0NmDp1arPvdaLPPvsMZ599NkJDQxEaGoqzzz4bGzdubPaYCRMm2K8ZWbVqFZ5++mn069cPfn5+OOecc1q99uLzzz/H2LFjERERgV69emH06NH417/+haNHj4rsBiIi/VOJiMil/fjjjyoA9YYbbmi2/YYbblDb+mvgt99+Uz08PNQbbrhBraqqUm02m/rpp5+qHh4e6uzZs5s9NiMjQwWgDhkyRF24cKFaW1urVlZWqv369VPfeecdVVVVFYB63333qQ0NDaqiKOq3336r+vj4qHfccUeL7z1+/Hg1Jiam1XE1fa/HHnus2falS5eqANSXXnpJbWxsVK1Wq/rSSy+pANRly5a12mPYsGHqu+++q1qtVrWoqEgdOXKkGhYWptbX1zd7rCzL6muvvababDZVVVX1888/V00mk31uREQ9Fc9IEBFRM6qq4qabboKvry9WrFgBb29vyLKMKVOm4Oabb8Y777yDXbt2tXheY2MjFi9eDA8PD/j4+ODFF1/E6aefDgAYM2YMnnnmGZhMJkiShPPPPx+333473njjDVRUVAiNNzs7Gw8++CDOPvts3HXXXTAajTAYDLjrrrswbtw4PPDAA8jOzm7xvKioKMyaNQsGgwHBwcGYN28eCgoKsG3bNvtjNm7cCEVRcP3110OWj/2VOXnyZMyePRt+fn5C4yYi0jsuJIiIqJmkpCQcOHAA48aNg4eHR7M/O+200wAAW7ZsafG8c845p9ldoC6++GIkJiYCAH777Te4ubk1e3y/fv1gtVpx4MABofF+/PHHaGxsxOWXX97iz6644go0Njba71x1vLPPPrvZf0dHRwNAs7cshYWFAQBuvvlmHD582L799ddfx5VXXik0biIivTM6ewBERKQthw4dAgBs3rwZ4eHhzf7MZrPBy8sLhYWFLZ7X9Et3a3755RcsXboUe/fuRUVFBWRZRl1dHQDY/6/oeCMiIlr8Wa9evZo95nghISHN/ttkMgE4dmalyfz58/Hnn39i/fr1WL9+PUaOHImrr74as2fPPul8iYh6Ap6RICKiVl177bXIz89v9r+ioiJUV1fjxRdfbPH4prf+nOiLL77A+PHjUV1dje+++w6FhYXIz8/HSy+91M0zOLm2xns8Ly8vfPzxx0hJScGjjz6K8vJyLFy4EP3798dPP/3kgFESEWkXFxJERNRMv379AKDNuxL99ttvrV5z0JaVK1dCVVUsXrwYMTExXTLG4zWNNzc3t8WfNc2h6TGdpSgKVFXFwIED8e9//xtpaWlYtWoVKioq8Mgjj5z6oImIXAAXEkREPVTT5zE0vZXnnXfewfr16zF8+HAMGDAAP/30E0pLS5s9Jz09HePGjUNJSUmHv4+7uzsAtPgU7bYWI15eXs3eXrRkyRJ8//33bX79q666Cm5ubti0aVOLP/vss8/g5uaGf/zjHx0e7/HmzJmD559/3v7fkiRh9uzZCAoKQnl5+Sl9TSIiV8GFBBFRDzVo0CAAwN69e1FbW4tnn30WeXl5kCQJK1euhCRJuOWWW+yLhsOHD+Paa6/FzJkzMWLEiA5/n2nTpgEAHn30URQXFwMAduzYgZdffrnNcRX+X3t3jKo4EMdx/L+dKAjCSIiQxjKFnU3wAnZaiIreIJ2VB7DS7omtkMLawivkAHoLES0EbQT9bRfYfbvwssvyeMv3U4aZyTCpvhCS08lOp5Odz2ebzWZ2u91+u34QBDafzy1NU3t7e7Pn82nP59OWy6WlaWrz+dyCIPjwfn+2Wq2y/0tIsiRJ7HK52Gg0+uM1AeC/8LlfnwUA/Ev1el2VSkVmpkKhIM/ztNvtJEn3+12DwUDValW+76vf7+t+v2dzD4eDOp2OnHPyfV9hGGqxWOjxeGRjJpOJnHMyM5VKJXmep81m824fSZKo0WioWCwqCAJ1Oh1Np1OZmSqViobDYTb2eDyq3W7LOadaraY4jvV6vdTtdt/d63q9ZvO2262iKJJzTs45RVGk7Xb7wz56vV52HuVyWa1WS5I0Ho9/eX2/3yuOY4VhKN/35Xmems2m1uv13z8cAPjivknSZ4YMAAAAgK+HV5sAAAAA5EZIAAAAAMiNkAAAAACQGyEBAAAAIDdCAgAAAEBuhAQAAACA3AgJAAAAALkREgAAAAByIyQAAAAA5EZIAAAAAMiNkAAAAACQGyEBAAAAILfvn+H2EV+/s34AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Test data:\")\n",
    "x_test = (np.array(x_test_data) / 255).reshape(len(x_test_data), 28*28, 1)\n",
    "format = lambda x : np.array([1 if i == x else 0 for i in range(10)]).reshape(10, 1)\n",
    "y_test = np.array([format(y) for y in y_test_data])\n",
    "\n",
    "def get_accuracy(network, x_data, y_data):\n",
    "    \"\"\"Returns accuracy and cost of the model on the given data.\"\"\"\n",
    "    out = net.run_batch(x_data)\n",
    "    cost = np.sum((out - y_data)**2 + reg/2 * y_data) / len(x_data)\n",
    "    \n",
    "    correct = 0\n",
    "    for x, y in zip(out, y_data):\n",
    "        if np.argmax(x) == np.argmax(y):\n",
    "            correct += 1\n",
    "    acc = correct / len(x_data)\n",
    "    \n",
    "    return acc, cost\n",
    "\n",
    "acc, cost = get_accuracy(net, x_test, y_test)\n",
    "print(f\"    Accuracy: {100 * acc: .3f}%\")\n",
    "print(f\"    Cost: {cost : .5f}\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Cost\")\n",
    "ax.plot(np.linspace(0, len(costs), num=len(costs)), costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9d3df1-09ea-48cb-9593-3372ec46f9b4",
   "metadata": {},
   "source": [
    "I left this thing training for just under 10 minutes (each epoch took about ~3 secs), and it got to 87% accuracy in that time. Not quite what 3b1b got to, but it's pretty easy to see that we could've gone higher if we let the AI train for longer. And that's not accounting for a lot of possibilities for optimizations here: running batches could be done with matrix multiplication instead of running each pattern separately, for example. We could also implement train many batches at once with multi-threading. And I didn't even mess with hyperparameters much.\n",
    "\n",
    "These are all things that are optimized in most AI libraries like Keras, though, so you usually don't need to worry too hard about them. At worst you'll have to tell them what kind of optimization you want, since there's usually many ways to optimize the same thing.\n",
    "\n",
    "But that's about it! We have an AI that can identify numbers from 0 to 9, 9 out of 10 times now. Cool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
